```python
from statsig_ai import Eval, EvalScorerArgs, EvalDataRecord, EvalHook

# Basic evaluation with a single scorer
result = Eval(
    name='greeting_task',
    data=[
        {'input': 'world', 'expected': 'Hello world'},
        {'input': 'test', 'expected': 'Hello test'},
    ],
    task=lambda input: f'Hello {input}',
    scorer=lambda args: args.output == args.expected,
    eval_run_name='run-123',
)

# Multiple named scorers
result2 = Eval(
    name='multi_scorer_task',
    data=[
        {'input': 'world', 'expected': 'Hello world'},
        {'input': 'test', 'expected': 'Hello test'},
    ],
    task=lambda input: f'Hello {input}',
    scorer={
        'correctness': lambda args: args.output == args.expected,
        'starts_with_hello': lambda args: args.output.startswith('Hello'),
        'length_check': lambda args: len(args.output) > 5,
    },
)

# Using parameters
def task_with_params(input: str, hook: EvalHook) -> str:
    prefix = hook.parameters.get('prefix', 'Hello')
    return f'{prefix} {input}'

result3 = Eval(
    name='parameterized_task',
    data=[
        {'input': 'world', 'expected': 'Hi world'},
    ],
    task=task_with_params,
    scorer=lambda args: args.output == args.expected,
    parameters={'prefix': 'Hi', 'suffix': '!', 'number': 123},
)

# Async task and scorer
async def async_task(input: str) -> str:
    import asyncio
    await asyncio.sleep(0.1)  # Simulate async work
    return f'Hello {input}'

async def async_scorer(args: EvalScorerArgs[str, str]) -> float:
    import asyncio
    await asyncio.sleep(0.05)  # Simulate async scoring
    return 1.0 if args.output == args.expected else 0.0

async def async_data():
    return [
        {'input': 'world', 'expected': 'Hello world'},
    ]

result4 = Eval(
    name='async_task',
    data=async_data,
    task=async_task,
    scorer=async_scorer,
)

# Using categories
result5 = Eval(
    name='categorized_task',
    data=[
        {'input': 'world', 'expected': 'Hello world', 'category': 'greeting'},
        {'input': 'test', 'expected': 'Hello test', 'category': ['greeting', 'test']},
    ],
    task=lambda input: f'Hello {input}',
    scorer=lambda args: args.output == args.expected,
)

# Summary scores
def summary_scorer(results):
    correct = sum(1 for r in results if r.scores.get('correctness', 0.0) == 1.0)
    return {
        'accuracy': correct / len(results) if results else 0.0,
        'total': len(results),
    }

result6 = Eval(
    name='summary_task',
    data=[
        {'input': 'world', 'expected': 'Hello world'},
        {'input': 'test', 'expected': 'Hello test'},
    ],
    task=lambda input: f'Hello {input}',
    scorer={
        'correctness': lambda args: args.output == args.expected,
    },
    summary_score_fn=summary_scorer,
)

# Data without expected values
result7 = Eval(
    name='no_expected_task',
    data=[
        {'input': 'world'},
        {'input': 'test'},
    ],
    task=lambda input: f'Hello {input}',
    scorer=lambda args: len(args.output) > 5,
)

# Error handling - task errors are caught
def failing_task(input: str) -> str:
    if input == 'fail':
        raise ValueError('Task failed')
    return f'Hello {input}'

result8 = Eval(
    name='error_handling_task',
    data=[
        {'input': 'world', 'expected': 'Hello world'},
        {'input': 'fail', 'expected': 'Error'},
    ],
    task=failing_task,
    scorer=lambda args: args.output != 'Error',
)
# Results will include error flags for failed tasks

# Using EvalDataRecord dataclass
result9 = Eval(
    name='dataclass_records',
    data=[
        EvalDataRecord(input='world', expected='Hello world'),
        EvalDataRecord(input='test', expected='Hello test'),
    ],
    task=lambda input: f'Hello {input}',
    scorer=lambda args: args.output == args.expected,
)

# Scorer with metadata
def metadata_scorer(args: EvalScorerArgs[str, str]) -> dict:
    if args.output == args.expected:
        return {
            'score': 1.0,
            'metadata': {'justification': 'exact match', 'confidence': 'high'}
        }
    return {'score': 0.0, 'metadata': {'justification': 'no match'}}

result10 = Eval(
    name='metadata_scorer',
    data=[
        {'input': 'world', 'expected': 'Hello world'},
    ],
    task=lambda input: f'Hello {input}',
    scorer=metadata_scorer,
)
```
