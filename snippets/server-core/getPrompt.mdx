### Getting a Prompt

Statsig can act as the control plane for your LLM prompts - allowing you to version and change them without deploying code. Learn more [here](/ai-evals/overview/). This API returns the full Prompt object, including all customized parameters such as model, temperature, topP, max tokens, and more. For example:

