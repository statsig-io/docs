"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[4532],{14859:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var a=n(74848),s=n(28453);const o={title:"Overview",sidebar_label:"Overview",slug:"/ai-evals/online-evals",keywords:["owner:vm"],last_update:{date:new Date("2025-09-23T00:00:00.000Z")}},i=void 0,r={id:"ai-evals/online-evals",title:"Overview",description:"What are Online Evals",source:"@site/docs/ai-evals/online-evals.mdx",sourceDirName:"ai-evals",slug:"/ai-evals/online-evals",permalink:"/ai-evals/online-evals",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/ai-evals/online-evals.mdx",tags:[],version:"current",lastUpdatedAt:17585856e5,frontMatter:{title:"Overview",sidebar_label:"Overview",slug:"/ai-evals/online-evals",keywords:["owner:vm"],last_update:{date:"2025-09-23T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Offline Evals",permalink:"/ai-evals/offline-evals"},next:{title:"Create an Online Eval",permalink:"/ai-evals/create-an-online-eval"}},l={},d=[{value:"What are Online Evals",id:"what-are-online-evals",level:2}];function u(e){const t={em:"em",h2:"h2",li:"li",ol:"ol",p:"p",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"what-are-online-evals",children:"What are Online Evals"}),"\n",(0,a.jsx)(t.p,{children:'Online evals let you grade your model output in production on real world use cases. You can run the "live" version of a prompt, but can also shadow run "candidate" versions of a prompt, without exposing users to them. Grading works directly on the model output, and has to work without a ground truth to compare against.'}),"\n",(0,a.jsx)(t.p,{children:"Steps to do this in Statsig -"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsx)(t.li,{children:"Create a Prompt. This contains the prompt for your task (e.g. Summarize ticket content. Don't include email addresses or credit cards in the summary). Create a v2 prompt that improves on this."}),"\n",(0,a.jsx)(t.li,{children:"In your app, use and produce model output using the v1 and v2 prompts. The output from v1 is rendered to the user; the output from v1 and v2 are judged by an LLM-as-a-judge."}),"\n",(0,a.jsx)(t.li,{children:"The grades from v1 and v2 are logged back to Statsig and can be compared there."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.em,{children:"Note: Online Evals is currently in beta. If you're interested in this feature, reach out to us on Slack!"})})]})}function c(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>r});var a=n(96540);const s={},o=a.createContext(s);function i(e){const t=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:t},e.children)}}}]);