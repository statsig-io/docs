"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[15301],{27706:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var n=s(74848),i=s(28453);const r={sidebar_label:"A/A Tests",title:"Running an A/A (aa) Test",keywords:["owner:vm"],last_update:{date:new Date("2025-09-18T00:00:00.000Z")}},a=void 0,o={id:"guides/aa-test",title:"Running an A/A (aa) Test",description:"In this guide, you will create and implement an A/A test on your product in Statsig from end to end. This is commonly used to validate a new experimentation engine you may be integrating with.",source:"@site/docs/guides/aa-test.mdx",sourceDirName:"guides",slug:"/guides/aa-test",permalink:"/guides/aa-test",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/guides/aa-test.mdx",tags:[],version:"current",lastUpdatedAt:17581536e5,frontMatter:{sidebar_label:"A/A Tests",title:"Running an A/A (aa) Test",keywords:["owner:vm"],last_update:{date:"2025-09-18T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Switchback Tests",permalink:"/experiments-plus/switchback-tests"},next:{title:"SEO Experimentation",permalink:"/guides/seo-testing"}},l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Create a feature gate in the console",id:"step-1-create-a-feature-gate-in-the-console",level:2},{value:"Step 2: Check the feature gate in your code",id:"step-2-check-the-feature-gate-in-your-code",level:2},{value:"Step 3: Review A/A test results",id:"step-3-review-aa-test-results",level:2},{value:"Simulated A/A Tests",id:"simulated-aa-tests",level:2},{value:"Offline A/A tests",id:"offline-aa-tests",level:3},{value:"File Description",id:"file-description",level:3}];function d(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"In this guide, you will create and implement an A/A test on your product in Statsig from end to end. This is commonly used to validate a new experimentation engine you may be integrating with."}),"\n",(0,n.jsx)(t.p,{children:"For new users just getting started with Statsig, we often recommend running an A/A test to provide a \u201clow-stakes\u201d first test environment to ensure that you have your metrics set up correctly and are seeing exposures flowing through as expected before kicking off your first real A/B test."}),"\n",(0,n.jsx)(t.p,{children:"By the end of this tutorial, you will have:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Created a new ",(0,n.jsx)(t.strong,{children:"Feature Gate"}),' in the Statsig console, set up as an "A/A test"']}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["You already have a ",(0,n.jsx)(t.a,{href:"https://console.statsig.com/sign_up",children:"Statsig account"})]}),"\n",(0,n.jsxs)(t.li,{children:["You already ",(0,n.jsx)(t.a,{href:"/sdks/quickstart",children:"integrated the Statsig Client SDK"})," into an existing application"]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"step-1-create-a-feature-gate-in-the-console",children:"Step 1: Create a feature gate in the console"}),"\n",(0,n.jsxs)(t.p,{children:["The easiest way to run an A/A test in Statsig is by leveraging a ",(0,n.jsx)(t.a,{href:"/feature-flags/overview",children:"Feature Gate"}),". You can also leverage an ",(0,n.jsx)(t.a,{href:"/guides/abn-tests",children:"Experiment"})," to run an A/A, but we chose to use a Feature Gate for this tutorial for simplicity."]}),"\n",(0,n.jsxs)(t.p,{children:["Log into the Statsig console at ",(0,n.jsx)(t.a,{href:"https://console.statsig.com/",children:"https://console.statsig.com/"})," and navigate to ",(0,n.jsx)(t.strong,{children:"Feature Gates"})," in the left-hand navigation panel."]}),"\n",(0,n.jsxs)(t.p,{children:["Click on the ",(0,n.jsx)(t.strong,{children:"Create"})," button and enter the name and (optional) description for your feature gate. We will call our feature gate \u201caatest_example\u201d. Click ",(0,n.jsx)(t.strong,{children:"Create"}),"."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163246908-24494f12-9d2e-4d8b-8e3e-4fc0ad9c7e41.png",alt:"create_new_fg_empty"})}),"\n",(0,n.jsxs)(t.p,{children:["In the Setup tab, define the rules for this feature gate. Tap ",(0,n.jsx)(t.strong,{children:"+ Add New Rule"}),". While you could run an A/A test on a specific user-group, platform, etc. the easiest setup is to simply divide all of your traffic 50/50 and deliver the same experience (your default product experience) to each group."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247089-360857f8-ada3-46af-ac82-e41fc99274b5.png",alt:"add_new_rule_empty"})}),"\n",(0,n.jsxs)(t.p,{children:["To do this, under ",(0,n.jsx)(t.strong,{children:"Criteria"})," select ",(0,n.jsx)(t.strong,{children:"Everyone"})," (you may need to scroll up), name your rule, and then change the ",(0,n.jsx)(t.strong,{children:"Pass Percentage"})," to 50%. Click ",(0,n.jsx)(t.strong,{children:"Add Rule"})," and that\u2019s it! Tap ",(0,n.jsx)(t.strong,{children:"Save Changes"})," in the upper right-hand corner."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247141-30c96f8a-8257-4b39-aa9f-830bb3c89228.png",alt:"add_new_rule_filled"})}),"\n",(0,n.jsx)(t.p,{children:"Your feature gate setup should now look as follows-"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247211-aacb2c54-1088-4c4a-ab7b-64e393383bdb.png",alt:"aa_rule_filled_out"})}),"\n",(0,n.jsx)(t.p,{children:"Check that it is working as expected by typing in some dummy user IDs into the console- roughly 50% of the time your IDs should pass, and 50% of the time they should fail."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247281-c0fb8089-f418-41af-a3a7-d8e684a3cdf3.png",alt:"check_rule_pass"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247287-7d565983-8253-4841-a65a-0f74d2e103b2.png",alt:"check_rule_fail"})}),"\n",(0,n.jsx)(t.h2,{id:"step-2-check-the-feature-gate-in-your-code",children:"Step 2: Check the feature gate in your code"}),"\n",(0,n.jsxs)(t.p,{children:["Copy the code snippet in the upper right hand corner of your feature gate page under the ",(0,n.jsx)(t.strong,{children:"< >"})," symbol and drop it into your application at the point you want to call the A/A check."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:'statsig.checkGate("aatest_example") \n'})}),"\n",(0,n.jsxs)(t.p,{children:["Now when a user renders this page in their client application, you will automatically start to see a live log stream in the Statsig console when you navigate to the ",(0,n.jsx)(t.strong,{children:"Diagnostics"})," tab for your feature gate."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247712-4610d7b4-188f-4418-a696-127b3c2f54da.png",alt:"logstream"})}),"\n",(0,n.jsx)(t.h2,{id:"step-3-review-aa-test-results",children:"Step 3: Review A/A test results"}),"\n",(0,n.jsxs)(t.p,{children:["Within 24 hours of starting your experiment, you'll see the cumulative exposures in the ",(0,n.jsx)(t.strong,{children:"Pulse Results"})," tab of your feature gate."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163247787-be1e816c-f715-4fd3-ad59-3a6caf48027a.png",alt:"cumulative_exposures"})}),"\n",(0,n.jsxs)(t.p,{children:["This will break down your logged exposures (as well as the distribution of the logged exposures). If something looks off, check the ",(0,n.jsx)(t.strong,{children:"Diagnostics"})," tab for more granular, day-by-day exposure breakdowns at both the Checks and User level."]}),"\n",(0,n.jsxs)(t.p,{children:["In the ",(0,n.jsx)(t.strong,{children:"Metric Lifts"})," panel, you can see the full picture of how all your tagged metrics are performing."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/163248267-7bd7419a-59e0-4d58-b8e5-8ace95ed74d9.png",alt:"pulse_results_empty"})}),"\n",(0,n.jsx)(t.p,{children:"What should you expect to see?"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Exposures"}),"- make sure you\u2019re seeing exposures flowing through as expected from your product. If you\u2019re not seeing exposures, use the ",(0,n.jsx)(t.strong,{children:"Diagnostics"})," tab and the ",(0,n.jsx)(t.strong,{children:"Exposure Stream"})," to debug"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Pulse results"}),"- roughly 5% of your metrics in Pulse should be showing a statistically significant change due to the 95% confidence interval of Statsig\u2019s stats engine"]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"We recommend running your A/A long enough to reach most of your weekly active users, or at least a week."}),"\n",(0,n.jsx)(t.h2,{id:"simulated-aa-tests",children:"Simulated A/A Tests"}),"\n",(0,n.jsx)(t.p,{children:"We\u2019ve made running A/A tests at scale easy by setting up simulated A/A tests that run every day in the background, for every company on the platform. An A/A test is like an A/B test - but both groups get the same experience. A/A tests help build trust in your experimentation platform (and your metrics!)"}),"\n",(0,n.jsxs)(t.p,{children:["A/A tests can be Online or Offline. An ",(0,n.jsx)(t.a,{href:"/guides/aa-test",children:"Online A/A test"})," is run on real users. An engineer instruments your app with the Statsig SDK to check for experiment assignment. Assignment is logged, but there's no difference in experience to the user."]}),"\n",(0,n.jsx)(t.p,{children:"Since there is no effect, you expect to only see statistical noise. When using 95% confidence intervals, only ~1 in 20 metrics will show a stat-sig difference between control and test."}),"\n",(0,n.jsx)(t.h3,{id:"offline-aa-tests",children:"Offline A/A tests"}),"\n",(0,n.jsx)(t.p,{children:"A single request runs on one unit type, and an offline A/A test works by"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsx)(t.li,{children:"Querying a representative sample of your data"}),"\n",(0,n.jsx)(t.li,{children:"Randomly assigning subjects to Test or Control"}),"\n",(0,n.jsx)(t.li,{children:"Computing relevant metrics for Test vs Control and running them through the stats engine"}),"\n",(0,n.jsx)(t.li,{children:"You're looking for the % of false positives. If your p-value cutoff is 0.05 (typical), you'd expect a ~5% false positive rate."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"You can download the running history of your simulated A/A test performance via the \u201cTools\u201d menu in your Statsig Console. We run 100 tests per request."}),"\n",(0,n.jsx)(t.h3,{id:"file-description",children:"File Description"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Column Name"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"metric_name"}),(0,n.jsx)(t.td,{children:"Name of the Metric"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"metric_type"}),(0,n.jsx)(t.td,{children:"Type of Metric"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"unit_type"}),(0,n.jsx)(t.td,{children:"The unit used to randomize (e.g. userID)"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"n_tests"}),(0,n.jsx)(t.td,{children:"The number of tests run"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"pct_ss_95_pct_confidence"}),(0,n.jsx)(t.td,{children:"The percentage of tests that have a stat-sig result for this metric"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"avg_units_per_test"}),(0,n.jsx)(t.td,{children:"The number of units (often users) sampled into the A/A test"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"avg_participating_units_per_test"}),(0,n.jsx)(t.td,{children:"The number of units in the test with a value for this metric"})]})]})]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:"https://user-images.githubusercontent.com/31516123/199562491-84d9b7c4-1cea-4308-a0a9-c04a14a41671.png",alt:"A/A test results table showing statistical significance percentages"})})]})}function u(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},28453:(e,t,s)=>{s.d(t,{R:()=>a,x:()=>o});var n=s(96540);const i={},r=n.createContext(i);function a(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);