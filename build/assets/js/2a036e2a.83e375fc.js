"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[49307],{87131:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var i=n(74848),s=n(28453);const o={title:"OpenAI",keywords:["owner:brock"],last_update:{date:new Date("2025-09-18T00:00:00.000Z")}},a=void 0,r={id:"integrations/openai",title:"OpenAI",description:"Context",source:"@site/docs/integrations/openai.md",sourceDirName:"integrations",slug:"/integrations/openai",permalink:"/integrations/openai",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/integrations/openai.md",tags:[],version:"current",lastUpdatedAt:17581536e5,frontMatter:{title:"OpenAI",keywords:["owner:brock"],last_update:{date:"2025-09-18T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Event Webhook",permalink:"/integrations/event_webhook"},next:{title:"Statsig Lite",permalink:"/experimentation/statsig-lite"}},l={},c=[{value:"Context",id:"context",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Code Breakdown",id:"code-breakdown",level:2},{value:"Initial Configuration",id:"initial-configuration",level:3},{value:"The ask_question Function",id:"the-ask_question-function",level:3},{value:"Tips for Using Statsig with AI",id:"tips-for-using-statsig-with-ai",level:3},{value:"Useful Links",id:"useful-links",level:3},{value:"Final code",id:"final-code",level:2}];function p(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"context",children:"Context"}),"\n",(0,i.jsx)(t.p,{children:'When using a pre-trained large language model, several inputs influence user experience, including the prompts used, the "inference parameters" given to the model, often including parameters like temperature, length penalties and repetition penalties, and even the exact model selected. Tools like Statsig can streamline the process of assigning users to various experiments, such as modifying these inputs, and can automatically identify when changes have a statistically significant impact on user experience metrics. This enables efficient iteration and optimization of user experience by tweaking model choices, prompts, and inference parameters. In this case we show how you can log both implicit indicators of user feedback (like response time) and more explicit ones, like self-reported satisfaction.'}),"\n",(0,i.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(t.p,{children:"The included Python code offers an example of the simple but powerful interaction between OpenAI's GPT and Statsig to experiment with model inputs and log user events. This example uses OpenAI's ChatCompletion feature to answer questions, plus a Statsig integration to experiment with model versions and log user feedback to the changes."}),"\n",(0,i.jsxs)(t.p,{children:['This example assumes you have a funded OpenAI account, plus a Statsig experiment that varies the model selected between "gpt-3.5-turbo" and "gpt-4". For more info on setting up a Statsig experiment, see the ',(0,i.jsx)(t.a,{href:"/experiments-plus",children:"experiments"})," page."]}),"\n",(0,i.jsx)(t.h2,{id:"code-breakdown",children:"Code Breakdown"}),"\n",(0,i.jsx)(t.h3,{id:"initial-configuration",children:"Initial Configuration"}),"\n",(0,i.jsx)(t.p,{children:"Of course, you'll need to install both the Statsig and OpenAI Python packages before starting:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bash",children:"pip3 install openai, statsig\n"})}),"\n",(0,i.jsx)(t.p,{children:"After that, we can begin coding in a python file:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import openai\nfrom statsig import statsig, StatsigEvent, StatsigUser\nimport time\n\nopenai.api_key = "your_openai_key"  # Replace with your own key\nstatsig.initialize("your_statsig_secret")  # Replace with your Statsig secret\nuser = StatsigUser("user-id") #This is a placeholder ID - in a normal experiment Statsig recommends using a user\'s actual unique ID for consistency in targeting. See https://docs.statsig.com/concepts/user\n'})}),"\n",(0,i.jsx)(t.h3,{id:"the-ask_question-function",children:"The ask_question Function"}),"\n",(0,i.jsxs)(t.p,{children:["The following code all occurs in one function titled ask_question (see the ",(0,i.jsx)(t.a,{href:"#final-code",children:"final code"}),")"]}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:"Get User Input"}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'#ask the user for a question to query GPT with\nquestion = input("\\nWhat is your question? ")\n'})}),"\n",(0,i.jsx)(t.p,{children:"First, we prompt the user with the question they'd like to ask ChatGPT"}),"\n",(0,i.jsxs)(t.ol,{start:"2",children:["\n",(0,i.jsx)(t.li,{children:"Query OpenAI's GPT"}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'\n#track the start time so we can check response time later\nstart_time = time.time()\n\ncompletion = openai.ChatCompletion.create(\n        model=statsig.get_experiment(user, "statsig_openai_integration").get("model", \'gpt-4\'),\n        messages=[\n            {"role": "system", "content": "You are a helpful assistant."},\n            {"role": "user", "content": question}\n        ]\n    )\n'})}),"\n",(0,i.jsx)(t.p,{children:"Next, we request a completion that queries the GPT model specified by the Statsig experiment (either gpt-3.5-turbo or gpt-4). We also start a timer so we can track the response time in our events later on."}),"\n",(0,i.jsxs)(t.ol,{start:"3",children:["\n",(0,i.jsx)(t.li,{children:"Display Response & Log Implicit Feedback"}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'#log "implicit" indicators to Statsig\nc = completion.choices[0]\nstats = completion.usage #completion object has the number of tokens used - which is that what GPT usage is charged on.\nstatsig.log_event(StatsigEvent(user, "chat_completion", value=c.finish_reason, metadata={"response_time": time.time() - start_time, "completion_tokens": stats["completion_tokens"], "prompt_tokens": stats["prompt_tokens"], "total_tokens": stats["total_tokens"]}))\n\n#print the message back to the user\nprint(f"\\nAnswer: {c.message[\'content\']}")\n'})}),"\n",(0,i.jsx)(t.p,{children:"With the response from OpenAI we have our first set of useful information to log to Statsig - like the response time and tokens used. We log this information with Statsig's SDK, storing them in the metadata of the request."}),"\n",(0,i.jsxs)(t.ol,{start:"4",children:["\n",(0,i.jsx)(t.li,{children:"Collect User Feedback & Log to Statsig"}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'#track explicit feedback: if the user is satisfied with the answer\nsatisfaction = input("\\nDid this answer your question? (y/n): ")\n\n# Log "explicit" indicators to Statsig\nif satisfaction == \'y\':\n    statsig.log_event(StatsigEvent(user, "satisfaction"))\nelif satisfaction == \'n\':\n    statsig.log_event(StatsigEvent(user, "dissatisfaction"))\n'})}),"\n",(0,i.jsx)(t.p,{children:"Next, we log a more explicit indicator of feedback, the user's self-reported satisfaction or dissatisfaction. The satisfaction metric can provide a strong indicator of the model's overall power."}),"\n",(0,i.jsx)(t.p,{children:"And we're done - we can run this Python program with the following code, now outside of our ask_question function."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'if __name__ == "__main__":\n    while input("Would you like to ask a question? (y/n): ").lower() == \'y\':\n        ask_question()\n'})}),"\n",(0,i.jsx)(t.h3,{id:"tips-for-using-statsig-with-ai",children:"Tips for Using Statsig with AI"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:"Experimentation: You can also test other model parameters like temperature, top_p, or initial prompts."}),"\n",(0,i.jsx)(t.li,{children:"Log Useful Data: Consider logging other interesting user interactions or feedback."}),"\n",(0,i.jsx)(t.li,{children:"Analyze and Iterate: After collecting enough data, analyze the results on the Statsig dashboard."}),"\n",(0,i.jsx)(t.li,{children:"User Identification: Consider integrating a mechanism to uniquely identify each user/session."}),"\n"]}),"\n",(0,i.jsx)(t.admonition,{type:"note",children:(0,i.jsx)(t.p,{children:"Always ensure you're in compliance with user privacy regulations and that you have user consent where necessary."})}),"\n",(0,i.jsx)(t.h3,{id:"useful-links",children:"Useful Links"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://platform.openai.com/docs/api-reference/chat/create",children:"OpenAI Documentation"})}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"final-code",children:"Final code"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'import openai\nfrom statsig import statsig, StatsigEvent, StatsigUser\nimport time\n\nopenai.api_key = "your_openai_key"\nstatsig.initialize("your_statsig_secret")\nuser = StatsigUser("user-id") #This is a placeholder ID - in a normal experiment Statsig recommends using a user\'s actual unique ID for consistency in targeting. See https://docs.statsig.com/concepts/user\n\ndef ask_question():\n\n    #ask the user for a question to query GPT with\n    question = input("\\nWhat is your question? ")\n\n    #track the start time so we can check response time later\n    start_time = time.time() \n    \n    #query GPT with the OpenAI Python library for chat completions\n    completion = openai.ChatCompletion.create(\n        model=statsig.get_experiment(user, "statsig_openai_collab").get("model", \'gpt-4\'), #experiment is setup to return either "gpt-3.5-turbo" or "gpt-4".\n        #other than varying the model selected, other attributes could be varied like "temperature", "top_p", "presence_penalty" and more. \n        #See the "Create chat completions" section of the OpenAI documentation for more: https://platform.openai.com/docs/api-reference/chat/create\n        messages=[\n            {"role": "system", "content": "You are a helpful assistant."}, #Initial prompts are another candidate for experimentation\n            {"role": "user", "content": question}\n        ]\n    )\n\n    #log "implicit" indicators to Statsig\n    c = completion.choices[0] #we\'ve only requested one choice, so selecting the first\n    stats = completion.usage #completion object has the number of tokens used - which is that what GPT usage is charged on.\n    statsig.log_event(StatsigEvent(user, "chat_completion", value=c.finish_reason, metadata={"response_time": time.time() - start_time, "completion_tokens": stats["completion_tokens"], "prompt_tokens": stats["prompt_tokens"], "total_tokens": stats["total_tokens"]}))\n\n    #print the message back to the user\n    print(f"\\nAnswer: {c.message[\'content\']}")\n    \n    #track explicit feedback: if the user is satisfied with the answer\n    satisfaction = input("\\nDid this answer your question? (y/n): ")\n\n    #log "explicit" indicators to Statsig\n    if satisfaction == \'y\':\n        statsig.log_event(StatsigEvent(user, "satisfaction"))\n    elif satisfaction == \'n\':\n        statsig.log_event(StatsigEvent(user, "dissatisfaction"))\n\nif __name__ == "__main__":\n    #Let the user abandon the question-asking process when they are done\n    while input("Would you like to ask a question? (y/n): ").lower() == \'y\':\n        ask_question()\n\n'})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>r});var i=n(96540);const s={},o=i.createContext(s);function a(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:t},e.children)}}}]);