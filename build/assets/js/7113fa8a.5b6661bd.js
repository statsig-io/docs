"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[70433],{42683:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>c,frontMatter:()=>s,metadata:()=>d,toc:()=>l});var i=n(74848),a=n(28453);const s={title:"Methodology",sidebar_label:"Methodology",slug:"/multi-armed-bandit",keywords:["owner:vm"],last_update:{date:new Date("2025-09-18T00:00:00.000Z")}},r=void 0,d={id:"autotune/multi-armed-bandit",title:"Methodology",description:"Model",source:"@site/docs/autotune/multi-armed-bandit.md",sourceDirName:"autotune",slug:"/multi-armed-bandit",permalink:"/multi-armed-bandit",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/autotune/multi-armed-bandit.md",tags:[],version:"current",lastUpdatedAt:17581536e5,frontMatter:{title:"Methodology",sidebar_label:"Methodology",slug:"/multi-armed-bandit",keywords:["owner:vm"],last_update:{date:"2025-09-18T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Monitoring",permalink:"/autotune/monitoring"},next:{title:"Contextual Bandit",permalink:"/autotune/contextual/introduction"}},o={},l=[{value:"Model",id:"model",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2}];function h(e){const t={a:"a",h2:"h2",li:"li",p:"p",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"model",children:"Model"}),"\n",(0,i.jsx)(t.p,{children:"The base Autotune implementation uses a Thompson Sampling (Bayesian) algorithm to estimate each variant's probability of being the best variant and allocate a proportional amount of traffic."}),"\n",(0,i.jsx)(t.p,{children:"For example, if a given variant has a 60% probability of being the best, Autotune will provide it 60% of the traffic. At a high level, the multi-armed bandit algorithm works by adding more users to a treatment as soon as it recognizes that it is clearly better in maximizing the reward (the target metric)."}),"\n",(0,i.jsx)(t.p,{children:"Throughout the process, higher performing treatments are allocated more traffic whereas underperforming treatments are allocated less traffic. When the winning treatment beats the second best treatment by a specified margin, the process terminates."}),"\n",(0,i.jsx)(t.p,{children:"Some helpful references:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://www.statsig.com/blog/introducing-autotune",children:"Statsig Blog"})}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf",children:"Goyal and Agrawal (Microsoft Research)"})," Regret Analysis"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://doordash.engineering/2022/03/15/using-a-multi-armed-bandit-with-thompson-sampling-to-identify-responsive-dashers/",children:"Doordash Engineering"})," Summary Blog"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"advantages",children:"Advantages"}),"\n",(0,i.jsx)(t.p,{children:'The major advantage of the base Multi-Armed Bandit over a contextual bandit is its ability to converge and identify a best variant. In cases where there is a "one-size-fits-all" solution, this is an elegant way to efficiently allocate traffic and determine a correct long-term solution while minimizing "regret" - the outcome of a high number of users being exposed to the worse variant like in an a/b test.'}),"\n",(0,i.jsx)(t.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,i.jsx)(t.p,{children:"The major disadvantages of a Multi-Armed Bandit as compared to a contextual bandit is its inability to personalize; in cases where there's interactions between a user's attributes and variants, vanilla Autotune can identify a global maxima that is worse than serving users their individual best variant."}),"\n",(0,i.jsx)(t.p,{children:'You can see this in a toy example; even if the "US Flag" variant had the highest overall CTR, it would be a bad choice for CA users. In this example, both groups will converge to a sub-optimal variant.'}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{}),(0,i.jsx)(t.th,{children:"A/B/n Test"}),(0,i.jsx)(t.th,{children:"Multi-Armed Bandit (Autotune)"}),(0,i.jsx)(t.th,{children:"Contextual Bandit (Autotune AI)"}),(0,i.jsx)(t.th,{children:"Ranking Engine"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Typical # Variants"}),(0,i.jsx)(t.td,{children:"2-3"}),(0,i.jsx)(t.td,{children:"4-8"}),(0,i.jsx)(t.td,{children:"4-8"}),(0,i.jsx)(t.td,{children:"Arbitrary #"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Personalization Factor"}),(0,i.jsx)(t.td,{children:"None"}),(0,i.jsx)(t.td,{children:"None"}),(0,i.jsx)(t.td,{children:"Moderate"}),(0,i.jsx)(t.td,{children:"High"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Input Data Required"}),(0,i.jsx)(t.td,{children:"None"}),(0,i.jsx)(t.td,{children:"Very Little (100+ samples)"}),(0,i.jsx)(t.td,{children:"Little - generally 1000+ samples"}),(0,i.jsx)(t.td,{children:"Tens of thousands to millions of samples"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Model Efficacy"}),(0,i.jsx)(t.td,{children:"None"}),(0,i.jsx)(t.td,{children:"Basic"}),(0,i.jsx)(t.td,{children:"Moderate"}),(0,i.jsx)(t.td,{children:"High"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Identifies Best Variant"}),(0,i.jsx)(t.td,{children:"Yes"}),(0,i.jsx)(t.td,{children:"Yes"}),(0,i.jsx)(t.td,{children:"No"}),(0,i.jsx)(t.td,{children:"No"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Consistent User Assignment"}),(0,i.jsx)(t.td,{children:"Yes"}),(0,i.jsx)(t.td,{children:"No"}),(0,i.jsx)(t.td,{children:"No"}),(0,i.jsx)(t.td,{children:"No"})]})]})]})]})}function c(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>d});var i=n(96540);const a={},s=i.createContext(a);function r(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);