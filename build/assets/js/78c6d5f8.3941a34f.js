"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[80958],{7989:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var s=t(74848),a=t(28453);const i={title:"Create an Online Eval",sidebar_label:"Create an Online Eval",slug:"/ai-evals/create-an-online-eval",last_update:{date:new Date("2025-09-23T00:00:00.000Z")}},o=void 0,r={id:"ai-evals/create-an-online-eval",title:"Create an Online Eval",description:"Create/analyze an online eval in 15 minutes",source:"@site/docs/ai-evals/create-an-online-eval.mdx",sourceDirName:"ai-evals",slug:"/ai-evals/create-an-online-eval",permalink:"/ai-evals/create-an-online-eval",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/ai-evals/create-an-online-eval.mdx",tags:[],version:"current",lastUpdatedAt:17585856e5,frontMatter:{title:"Create an Online Eval",sidebar_label:"Create an Online Eval",slug:"/ai-evals/create-an-online-eval",last_update:{date:"2025-09-23T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Overview",permalink:"/ai-evals/online-evals"},next:{title:"Workspace Management Overview",permalink:"/access-management/introduction"}},l={},c=[{value:"Create/analyze an online eval in 15 minutes",id:"createanalyze-an-online-eval-in-15-minutes",level:2}];function d(e){const n={code:"code",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"createanalyze-an-online-eval-in-15-minutes",children:"Create/analyze an online eval in 15 minutes"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Identify your Prompt Set"})}),"\n",(0,s.jsx)(n.p,{children:"In Prompts, there are three prompt types: Live, Candidate, and Archive. Before starting an online evaluation, it\u2019s important to organize your prompt versions into these categories:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Live"})," prompt is the version actively served to users."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Candidate"})," prompts are not shown to users but run in the background. The user\u2019s input is still processed against them, and their outputs are logged and graded alongside the live version."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Archive"})," prompts are inactive versions that are not served and kept offline."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Your prompt set will comprise of the Live version and Candidate versions."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Image",src:t(23799).A+"",width:"1353",height:"769"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Load your prompt set in code and run completions on user input"})}),"\n",(0,s.jsx)(n.p,{children:"In the example below, we demonstrate how to integrate your prompt set into an application using the Statsig SDKs. Any macros in your prompts will be replaced with the user input. The live version of the prompt is extracted and served to the user, with completions run on it. At the same time, completions are also run in the background on the shadow (candidate) prompts for evaluation and comparison."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:'const promptSet = statsig.getPromptSet("ai_config_name", userInput);\n\n// get the live prompt\nconst livePrompt = promptSet.getLive();\n\n// get the shadow prompts\nconst shadowPrompts = promptSet.getShadows();\n\n// run completions on your live prompt and show the output to the user\nconst liveOutput = client.completions.create(my_model, livePrompt);\n\n// simulateneously run completions on the shadow prompts to get their output\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Score your output using graders"})}),"\n",(0,s.jsx)(n.p,{children:"Once you have a completion\u2019s output, it should be evaluated using a grader\u2014either one created in Statsig or a custom grader of your choice. The resulting score should always fall within the range of 0 to 1."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"4. Log Eval Results to Statsig"})}),"\n",(0,s.jsx)(n.p,{children:"You can log your scores as events in Statsig to see the results in your console"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:'// log results for the live version\nStatsig.logEvent("<grader_name>", "<ai_config_name>", {\n  score: "<live_version_score> [0-1]",\n  version_name: "<version_name>",\n});\n\n// log results for all the shadow versions too\nStatsig.logEvent("<grader_name>", "<ai_config_name>", {\n  score: "<shadow_version_score>",\n  version_name: "<version_name>",\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"5. View Results in Statsig"})}),"\n",(0,s.jsx)(n.p,{children:"You can now view these results in Statsig! Select the version you want to evaluate and the versions you want to compare it against. This end to end online eval helps you iterate on your prompts and gain valuable insights."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Image",src:t(52559).A+"",width:"1309",height:"879"})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},52559:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/online-results-a10f7bac35607c0caf8f3b9bee549778.png"},23799:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/version-types-a6d308e31f95a7581a6629fe906a53d2.png"},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(96540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);