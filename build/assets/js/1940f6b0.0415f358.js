"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[13747,14386],{18518:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>d});var i=n(74848),s=n(28453),r=n(43896);const a={title:"Experiment Quality Score",sidebar_label:"Quality Score",slug:"/experimentation/quality-score-whn",keywords:["owner:vm"],last_update:{date:new Date("2025-09-18T00:00:00.000Z")}},o=void 0,l={id:"experiments/quality-score-whn",title:"Experiment Quality Score",description:"",source:"@site/docs/experiments/quality-score-whn.mdx",sourceDirName:"experiments",slug:"/experimentation/quality-score-whn",permalink:"/experimentation/quality-score-whn",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/experiments/quality-score-whn.mdx",tags:[],version:"current",lastUpdatedAt:17581536e5,frontMatter:{title:"Experiment Quality Score",sidebar_label:"Quality Score",slug:"/experimentation/quality-score-whn",keywords:["owner:vm"],last_update:{date:"2025-09-18T00:00:00.000Z"}},sidebar:"warehouse",previous:{title:"Normalized Metrics",permalink:"/statsig-warehouse-native/metrics/normalized-metrics"},next:{title:"Loading Pulse",permalink:"/statsig-warehouse-native/features/reloads"}},c={},d=[...r.toc];function h(e){return(0,i.jsx)(r.default,{})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h()}},43896:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var i=n(74848),s=n(28453);const r={title:"Experiment Quality Score",sidebar_label:"Quality Score",slug:"/experimentation/quality-score",keywords:["owner:vm"],last_update:{date:new Date("2025-09-18T00:00:00.000Z")}},a=void 0,o={id:"experiments/quality-score",title:"Experiment Quality Score",description:"Introduction",source:"@site/docs/experiments/quality-score.mdx",sourceDirName:"experiments",slug:"/experimentation/quality-score",permalink:"/experimentation/quality-score",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/experiments/quality-score.mdx",tags:[],version:"current",lastUpdatedAt:17581536e5,frontMatter:{title:"Experiment Quality Score",sidebar_label:"Quality Score",slug:"/experimentation/quality-score",keywords:["owner:vm"],last_update:{date:"2025-09-18T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Overrides",permalink:"/experiments-plus/overrides"},next:{title:"Implement",permalink:"/experiments-plus/implement"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Advanced Configuration",id:"advanced-configuration",level:2},{value:"Calculation Notes",id:"calculation-notes",level:2},{value:"Viewing Quality Scores",id:"viewing-quality-scores",level:2}];function d(e){const t={code:"code",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(t.p,{children:"The Experiment Quality Score is a metric designed to get a quick understanding of the quality - and trustworthiness - of an experiment configured within Statsig."}),"\n",(0,i.jsx)(t.p,{children:"This helps experimenters and their peers across an organization quickly identify potential issues in experiment setup, execution, and data collection, ensuring more\nconfident decision-making."}),"\n",(0,i.jsx)(t.p,{children:"Measuring this score over all experiments can help teams to discover systematic issues in their program, and point out opportunities to mature\ntheir experimentation program over time."}),"\n",(0,i.jsx)(t.h2,{id:"configuration",children:"Configuration"}),"\n",(0,i.jsx)(t.p,{children:"Experimentation quality score can be enabled in the project settings under Settings -> Experimentation -> Experiment Quality Score."}),"\n",(0,i.jsx)(t.p,{children:"There is a list of pre-defined assessment criteria used. The weights of each criteria can be customized based on an organization's needs,\nthough Statsig provides default values."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/91fea3df-0dae-4d0a-ada8-1c1c9313e60e",alt:"Experiment quality score configuration interface"})}),"\n",(0,i.jsx)(t.h2,{id:"advanced-configuration",children:"Advanced Configuration"}),"\n",(0,i.jsx)(t.p,{children:"For more sophisticated customers there may be additional checks desired, different product teams with different needs,\nor it may be the case that the default thresholds aren't correct\nfor their use case. For example, maybe hypotheses should be at least 200 characters, AND contain a link to an external planning doc."}),"\n",(0,i.jsxs)(t.p,{children:["This can be managed easily through the Statsig console API. By running a POST or PATCH on the ",(0,i.jsx)(t.code,{children:"console/v1/experiments"})," endpoint,\none can update the individual scores on any given experiment. Targeting the existing set of scores allows overriding weights\n(usually to 0), meaning the list can be just the custom set desired."]}),"\n",(0,i.jsx)(t.p,{children:"For example, running patch on an experiment with this payload:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'{\n    "manualQualityScores": [\n        {\n            "criteriaName": "HYPOTHESIS_LENGTH",\n            "criteriaDescription": "Check passed",\n            "status": "PASSED",\n            "score": 0,\n            "weight": 0\n        },\n        {\n            "criteriaName": "MyCompany\\\'s Hypothesis Check",\n            "criteriaDescription": "Has Internal URL and > 200 Chars",\n            "status": "PASSED",\n            "score": 100,\n            "weight": 100\n        },\n        {\n            "criteriaName": "Naming",\n            "criteriaDescription": "Experiment prefixed with team name",\n            "status": "FAILED",\n            "score": 0,\n            "weight": 100\n        }\n    ]\n}\n'})}),"\n",(0,i.jsx)(t.p,{children:"Would:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Drop the original HYPOTHESIS_LENGTH check"}),"\n",(0,i.jsx)(t.li,{children:"Keep the other original checks, with their weights"}),"\n",(0,i.jsxs)(t.li,{children:["Add a new check, ",(0,i.jsx)(t.code,{children:"MyCompany's Hypothesis Check"}),", for custom logic on the hypothesis"]}),"\n",(0,i.jsxs)(t.li,{children:["Add a new check, ",(0,i.jsx)(t.code,{children:"Naming"}),", for custom logic on the name"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Note that the other weights would be normalized. If the original HYPOTHESIS_LENGTH had a weight of 10, the total weight would\nnow be 290 and scores normalized accordingly. If all of the non-custom checks were passing, the score would be 190/290 or ~66%"}),"\n",(0,i.jsx)(t.p,{children:"The general flow for using this approach is to:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Use Console API's ",(0,i.jsx)(t.code,{children:"experiments/get"})," to pull all experiments"]}),"\n",(0,i.jsxs)(t.li,{children:["For Each Experiment","\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Run custom logic"}),"\n",(0,i.jsx)(t.li,{children:"Patch results"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"calculation-notes",children:"Calculation Notes"}),"\n",(0,i.jsxs)(t.p,{children:["Checks which are in an unready state will be skipped during evaluation, and the other weights will be renormalized to 100%. For example, if the experiment has not started,\nthe ",(0,i.jsx)(t.code,{children:"Balanced Exposures"})," component will be in an unready state and ignored."]}),"\n",(0,i.jsx)(t.p,{children:"Checks with a weight of 0 will be omitted entirely from the card."}),"\n",(0,i.jsx)(t.h2,{id:"viewing-quality-scores",children:"Viewing Quality Scores"}),"\n",(0,i.jsx)(t.p,{children:"When enabled, Quality Scores will show up in the details tab of an experiment. Applicable checks will be evaluated and contribute to the number shown."}),"\n",(0,i.jsx)(t.p,{children:"The score will be color-coded based on the % threshold it is at."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:">= 85% corresponds to passing/green"}),"\n",(0,i.jsx)(t.li,{children:">= 50% corresponds to warning/yellow"}),"\n",(0,i.jsx)(t.li,{children:"< 50% corresponds to error/red."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/d7483b96-8077-419d-9e23-a3e648b7e066",alt:"Experiment quality score display with color-coded status"})}),"\n",(0,i.jsx)(t.p,{children:"Quality scores are also available through the console API. This allows bulk scrapes of the data for easy analysis."})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(96540);const s={},r=i.createContext(s);function a(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);