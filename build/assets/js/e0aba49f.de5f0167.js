"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[80339],{8106:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var i=n(74848),s=n(28453);const r={title:"Create an experiment",sidebar_label:"Create",slug:"/experiments-plus/create-new",keywords:["owner:vm"],last_update:{date:new Date("2025-10-02T00:00:00.000Z")}},a=void 0,o={id:"experiments-plus/create-new",title:"Create an experiment",description:"You're viewing the Cloud docs for this page. Metrics and experiments behave differently in Warehouse Native. Read about Configuring Experiments in Warehouse Native.",source:"@site/docs/experiments-plus/create-new.md",sourceDirName:"experiments-plus",slug:"/experiments-plus/create-new",permalink:"/experiments-plus/create-new",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/experiments-plus/create-new.md",tags:[],version:"current",lastUpdatedAt:17593632e5,frontMatter:{title:"Create an experiment",sidebar_label:"Create",slug:"/experiments-plus/create-new",keywords:["owner:vm"],last_update:{date:"2025-10-02T00:00:00.000Z"}},sidebar:"cloud",previous:{title:"Experiments Overview",permalink:"/experiments-plus"},next:{title:"Overrides",permalink:"/experiments-plus/overrides"}},l={},c=[{value:"User-level Experiments",id:"user-level-experiments",level:3},{value:"Configure Your Scorecard",id:"configure-your-scorecard",level:3},{value:"Configure Allocation and Targeting",id:"configure-allocation-and-targeting",level:3},{value:"Allocation",id:"allocation",level:4},{value:"Targeting",id:"targeting",level:4},{value:"Configure Your Groups and Parameters",id:"configure-your-groups-and-parameters",level:3},{value:"Device-level and Custom ID Experiments",id:"device-level-and-custom-id-experiments",level:3},{value:"ID Mapping Capabilities",id:"id-mapping-capabilities",level:2},{value:"Isolated Experiments",id:"isolated-experiments",level:3},{value:"Significance Level Adjustments",id:"significance-level-adjustments",level:3},{value:"Target Duration",id:"target-duration",level:3},{value:"Hypothesis Advisor",id:"hypothesis-advisor",level:3}];function d(e){const t={a:"a",admonition:"admonition",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.admonition,{title:"Warehouse Native users",type:"tip",children:(0,i.jsxs)(t.p,{children:["You're viewing the Cloud docs for this page. Metrics and experiments behave differently in Warehouse Native. Read about ",(0,i.jsx)(t.a,{href:"/statsig-warehouse-native/features/experiment-options",children:"Configuring Experiments in Warehouse Native"}),"."]})}),"\n",(0,i.jsxs)(t.p,{children:["This doc walks through the steps of creating a new experiment in the Statsig console. If you're looking for an end-to-end guide that includes integrating the Statsig SDK, see ",(0,i.jsx)(t.a,{href:"/guides/abn-tests",children:"Run your first experiment"}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"user-level-experiments",children:"User-level Experiments"}),"\n",(0,i.jsx)(t.p,{children:"To create a user-level experiment, follow these steps:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Log into the Statsig console at ",(0,i.jsx)(t.a,{href:"https://console.statsig.com/",children:"https://console.statsig.com/"})]}),"\n",(0,i.jsxs)(t.li,{children:["Navigate to ",(0,i.jsx)(t.strong,{children:"Experiments"})," in the left-hand navigation panel"]}),"\n",(0,i.jsxs)(t.li,{children:["Click on the ",(0,i.jsx)(t.strong,{children:"Create"})," button"]}),"\n",(0,i.jsx)(t.li,{children:"Enter the name and description for your experiment as shown in the figure below"}),"\n",(0,i.jsxs)(t.li,{children:["By default, your experiment runs in its own ",(0,i.jsx)(t.strong,{children:"Layer"}),". A Layer allows you to manage multiple experiments and feature flags together. If you want to add this experiment to an existing Layer, select ",(0,i.jsx)(t.strong,{children:"Add Layer"})," under ",(0,i.jsx)(t.strong,{children:"Advanced"})," in the experiment creation modal. You can also create a new Layer by selecting ",(0,i.jsx)(t.strong,{children:"Create New Layer"}),"."]}),"\n",(0,i.jsxs)(t.li,{children:["Click ",(0,i.jsx)(t.strong,{children:"Create"})]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/f7c0bdd7-8fdf-485a-b90f-e2aa619149c4",alt:"Experiment creation modal interface"})}),"\n",(0,i.jsx)(t.h3,{id:"configure-your-scorecard",children:"Configure Your Scorecard"}),"\n",(0,i.jsxs)(t.p,{children:["When running an experiment, it\u2019s common to test a specific hypothesis using a set of key metrics. The ",(0,i.jsx)(t.strong,{children:"Scorecard"})," feature makes this easy by letting you enter your hypothesis and select both primary and secondary metrics."]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Primary Metrics"})," are those you expect to be directly impacted by the experiment."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Secondary Metrics"})," are important to monitor to ensure there are no unintended side effects, but they aren\u2019t the primary focus of your experiment."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["Configuring the Scorecard is a required step when creating an experiment. It provides your team with clear context on what is being tested and how success is measured. You must enter your hypothesis and select at least one primary metric. Metrics added to the Scorecard are computed daily and eligible for advanced treatments like ",(0,i.jsx)(t.a,{href:"/stats-engine/methodologies/cuped",children:"CUPED"})," and ",(0,i.jsx)(t.a,{href:"/experiments-plus/sequential-testing#what-is-sequential-testing",children:"Sequential Testing"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["For best practices on configuring your Scorecard, read more ",(0,i.jsx)(t.a,{href:"/pulse/read-pulse",children:"here"}),"."]}),"\n",(0,i.jsx)("img",{width:"1086",alt:"Screenshot 2025-02-06 at 6 34 00 PM",src:"https://github.com/user-attachments/assets/cf3e3be7-61d5-4079-bb0f-6b6685f72f01"}),"\n",(0,i.jsx)(t.h3,{id:"configure-allocation-and-targeting",children:"Configure Allocation and Targeting"}),"\n",(0,i.jsx)(t.p,{children:"This is where most of your experiment configuration happens."}),"\n",(0,i.jsx)(t.h4,{id:"allocation",children:"Allocation"}),"\n",(0,i.jsxs)(t.p,{children:["For ",(0,i.jsx)(t.strong,{children:"Allocation"}),", enter the percentage of users you want to assign to this experiment. You can allocate up to 100% of eligible users, but it\u2019s good practice to start with a smaller percentage, verify the experiment\u2019s stability, and then ramp up the allocation."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/203620564-028c7244-c77b-4f51-92e1-40f522a03902.png",alt:"Experiment allocation configuration interface"})}),"\n",(0,i.jsx)(t.h4,{id:"targeting",children:"Targeting"}),"\n",(0,i.jsxs)(t.p,{children:["To configure ",(0,i.jsx)(t.strong,{children:"Targeting"})," criteria, click to edit the ",(0,i.jsx)(t.strong,{children:"Targeting"})," section. You can either set new targeting criteria or use an existing ",(0,i.jsx)(t.strong,{children:"Feature Gate"}),". This will limit the experiment to only the users who meet the defined conditions."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/6f20dfbc-725f-4384-bd06-1fe23c15fcf6",alt:"Experiment targeting configuration interface"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:'If your targeting is straightforward, creating it through Inline Targeting works well. (Click "Criteria: Everyone" to get started.)'}),"\n",(0,i.jsxs)(t.li,{children:["For more advanced targeting (e.g., progressive rollouts) or if you want to maintain targeting criteria when you launch your experiment, it\u2019s better to reference an existing ",(0,i.jsx)(t.strong,{children:"Feature Gate"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["By default, no targeting criteria are set, so your experiment will include all allocated users within the defined ",(0,i.jsx)(t.strong,{children:"Layer"})," or exposed user base."]}),"\n",(0,i.jsx)(t.h3,{id:"configure-your-groups-and-parameters",children:"Configure Your Groups and Parameters"}),"\n",(0,i.jsxs)(t.p,{children:["When configuring ",(0,i.jsx)(t.strong,{children:"Groups and Parameters"}),", it\u2019s a good idea to define your parameters first. These are the variables that control the behavior of the different experiment variants."]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Enter the values the experiment parameter will take for each variant. For more about the difference between ",(0,i.jsx)(t.strong,{children:"Groups"})," and ",(0,i.jsx)(t.strong,{children:"Parameters"}),", refer to ",(0,i.jsx)(t.a,{href:"/experiments-plus/getting-group",children:"Groups vs. Parameters"}),"."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:'You can add additional groups by clicking the "+" next to the existing groups. The user allocation will automatically adjust as you add more groups.'}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://user-images.githubusercontent.com/101903926/203623897-5ae52609-80cc-4927-a64b-5e3af0005fd0.png",alt:"Experiment groups and parameters configuration interface"})}),"\n",(0,i.jsxs)(t.p,{children:["In addition, you can name, describe, and even add variant images for each group under the ",(0,i.jsx)(t.strong,{children:"Groups"})," section. However, only the parameters and values will affect what users see\u2014group names and descriptions are not used in the experiment code."]}),"\n",(0,i.jsx)(t.h3,{id:"device-level-and-custom-id-experiments",children:"Device-level and Custom ID Experiments"}),"\n",(0,i.jsxs)(t.p,{children:["By default, experiments randomize users based on ",(0,i.jsx)(t.strong,{children:"User ID"}),'. If you need to use a different ID type (e.g., device-level), follow steps 1\u20134 from the "User-level Experiments" section, then:']}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Click the ",(0,i.jsx)(t.strong,{children:"ID Type"})," dropdown menu and choose the desired ID type."]}),"\n",(0,i.jsxs)(t.li,{children:["Click ",(0,i.jsx)(t.strong,{children:"Create"})]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/8be2c2b5-d9ad-49cb-81c0-a48dfab0a158",alt:"ID type selection dropdown interface"})}),"\n",(0,i.jsx)(t.p,{children:"Afterward, continue with the same steps described above to finish configuring the experiment."}),"\n",(0,i.jsx)(t.h2,{id:"id-mapping-capabilities",children:"ID Mapping Capabilities"}),"\n",(0,i.jsx)(t.p,{children:"When running experiments, you may want to start with one ID type (like stableID for device-level targeting) but analyze results using events from another ID type (like userID for logged-in user metrics)."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Warehouse Native"}),": Supports ID mapping between different identifier types (e.g., stableID to userID) through Entity Property Source configuration."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Cloud"}),": Currently does not support mapping between different ID types. Experiments started with stableID will only analyze events with stableID, and experiments started with userID will only analyze events with userID."]}),"\n",(0,i.jsx)(t.p,{children:"For advanced ID mapping requirements, consider using Statsig Warehouse Native."}),"\n",(0,i.jsx)(t.h3,{id:"isolated-experiments",children:"Isolated Experiments"}),"\n",(0,i.jsx)(t.p,{children:'If you want to create an experiment that excludes users exposed to other experiments, follow steps 1\u20134 from the "User-level Experiments" section. Then:'}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Select ",(0,i.jsx)(t.strong,{children:"Advanced"})," options."]}),"\n",(0,i.jsxs)(t.li,{children:["Select an existing ",(0,i.jsx)(t.strong,{children:"Layer"})," or create a new one."]}),"\n",(0,i.jsxs)(t.li,{children:["Click ",(0,i.jsx)(t.strong,{children:"Create"}),"."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/09f23cb7-284d-4504-9a22-85eb7cc51534",alt:"Isolated experiment layer configuration interface"})}),"\n",(0,i.jsx)(t.p,{children:"Now, complete the rest of the experiment setup as described above."}),"\n",(0,i.jsx)(t.h3,{id:"significance-level-adjustments",children:"Significance Level Adjustments"}),"\n",(0,i.jsx)(t.p,{children:"By default, Experiment Results display with 95% confidence intervals and without Bonferroni correction. This can be customized during experiment setup or later when viewing results in Experiment Results."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Bonferroni Correction:"})," Apply this to reduce the risk of false positives in experiments with multiple test groups. The significance level (",(0,i.jsx)(t.em,{children:"\u03b1"}),") is divided by the number of test variants."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Default Confidence Interval:"})," Choose a lower confidence interval (e.g., 80%) if you prefer faster results with higher tolerance for false positives, or stick with 95% for greater certainty."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/a6019d56-5c7f-43de-9679-dbf3579483e1",alt:"Significance level adjustment settings interface"})}),"\n",(0,i.jsx)(t.h3,{id:"target-duration",children:"Target Duration"}),"\n",(0,i.jsxs)(t.p,{children:["Setting a target duration is optional, but it helps ensure that you wait long enough for the experiment to reach full power. You can set the target as either a specific number of days or a number of exposures, and use the ",(0,i.jsx)(t.a,{href:"/experiments-plus/power-analysis",children:(0,i.jsx)(t.strong,{children:"Power Analysis Calculator"})})," to determine what target works best for your metrics."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Target duration setting interface",src:n(49785).A+"",width:"1731",height:"216"})}),"\n",(0,i.jsxs)(t.p,{children:["\ud83d\udca1 ",(0,i.jsx)(t.strong,{children:"Target durations longer than 90 days:"})," By default, Statsig computes Experiment Results results for the first 90 days, though the experiment itself can run longer. Before setting a duration beyond 90 days, ask yourself if results past that period will still be relevant, and if earlier data might already provide the insights you need."]}),"\n",(0,i.jsx)("img",{alt:"Hypothesis Advisor Screenshot",src:"https://github.com/user-attachments/assets/371f7de7-f428-41d5-ad0e-8fdf9d223982"}),"\n",(0,i.jsx)(t.p,{children:"Once set, you can track progress against the target duration/exposures in the experiment header. You\u2019ll also receive notifications via email and Slack (if integrated) when the target is reached."}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h3,{id:"hypothesis-advisor",children:"Hypothesis Advisor"}),"\n",(0,i.jsx)(t.p,{children:"Writing good experiment hypotheses is key to a strong experimentation culture. Statsig now gives instant feedback on experiment hypotheses\u2014flagging what\u2019s missing. Admins can set custom requirements, which Statsig uses to guide experimenters toward stronger, more complete hypotheses."}),"\n",(0,i.jsx)("img",{alt:"Screenshot showing the Hypothesis Advisor",src:"https://github.com/user-attachments/assets/ef70cbf1-db6b-4c15-81fd-ebabbda12e83"}),"\n",(0,i.jsx)(t.p,{children:'This Statsig AI feature is default disabled and has to be enabled for your project.\nDo this from Settings -> Experiment -> Project -> Statsig AI.\nThis is also where you configure any custom requirements you want Hypothesis Advisor to ensure adherence to (e.g. "Strongly recommend that a validation plan be mentioned").'}),"\n",(0,i.jsx)("img",{alt:"Setting showing how to enable Statsig AI",src:"/img/ai/hypothesis-advisor-enable-settings.png"})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},49785:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/target_duration_setting-05efa379cd95cb1832593190e31a3eb8.png"},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(96540);const s={},r=i.createContext(s);function a(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);