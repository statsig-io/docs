"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[39511],{79431:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var i=s(74848),n=s(28453);const a={title:"Setup Checklist",slug:"/statsig-warehouse-native/guides/checklist",sidebar_label:"Setup Checklist",keywords:["owner:vm"],last_update:{date:new Date("2025-09-18T00:00:00.000Z")}},r=void 0,o={id:"statsig-warehouse-native/guides/checklist",title:"Setup Checklist",description:"After you've connected your warehouse and set up both metrics and assignment sources, you can ensure your setup is complete and correct by checking the following items:",source:"@site/docs/statsig-warehouse-native/guides/checklist.md",sourceDirName:"statsig-warehouse-native/guides",slug:"/statsig-warehouse-native/guides/checklist",permalink:"/statsig-warehouse-native/guides/checklist",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/statsig-warehouse-native/guides/checklist.md",tags:[],version:"current",lastUpdatedAt:17581536e5,frontMatter:{title:"Setup Checklist",slug:"/statsig-warehouse-native/guides/checklist",sidebar_label:"Setup Checklist",keywords:["owner:vm"],last_update:{date:"2025-09-18T00:00:00.000Z"}},sidebar:"warehouse",previous:{title:"Running Email Experiments",permalink:"/statsig-warehouse-native/guides/email-experiments"},next:{title:"Debugging",permalink:"/statsig-warehouse-native/guides/debugging"}},l={},c=[{value:"1. Primary keys",id:"1-primary-keys",level:2},{value:"2. Timestamps",id:"2-timestamps",level:2},{value:"3. Exposure duplication",id:"3-exposure-duplication",level:2},{value:"4. Data availability",id:"4-data-availability",level:2}];function u(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:"After you've connected your warehouse and set up both metrics and assignment sources, you can ensure your setup is complete and correct by checking the following items:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:"Primary keys"}),"\n",(0,i.jsx)(t.li,{children:"Timestamps"}),"\n",(0,i.jsx)(t.li,{children:"Duplication"}),"\n",(0,i.jsx)(t.li,{children:"Data availability"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Once you've completed these checks, your offline results should align with those in the Statsig console when advanced features are disabled."}),"\n",(0,i.jsx)(t.h2,{id:"1-primary-keys",children:"1. Primary keys"}),"\n",(0,i.jsx)(t.p,{children:"When setting up an experiment, you can select the unit of assignment, acting as the primary key to join the assignment with metrics. The assignment source and the metrics source must use the same primary key."}),"\n",(0,i.jsx)(t.p,{children:"In an Analyze-Only experiment, this primary key can be selected from the unit IDs defined by your assignment source."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Ensure the unit ID in your assignment source matches the unit ID in your metrics source."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"In an Assign and Analyze experiment, the primary key (unit ID) is generated by the Statsig SDK."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"You can verify this unit ID in the statsig_forwarded_exposures table within the assignment sources."}),"\n",(0,i.jsxs)(t.li,{children:["You must either forward the unit ID to the SDK (",(0,i.jsx)(t.a,{href:"/client/introduction",children:"docs"}),") or utilize the SDK to manage your features and correspondingly generate the metrics table."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"2-timestamps",children:"2. Timestamps"}),"\n",(0,i.jsx)(t.p,{children:"It is important to analyze metric data only after a user has been exposed to the experiment. Pre-experiment data should have no average treatment effect, and therefore its inclusion dilutes results. Statsig employs a timestamp-based join for this purpose, with an option for a date-based join for daily data. This should look like the SQL snippet below:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"WITH \nmetrics as (...),\nexposures as (...),\njoined_data as (\n    SELECT \n        exposures.unit_id,\n        exposures.experiment_id,\n        exposures.group_id,\n        metrics.timestamp,\n        metrics.value\n    FROM exposures\n    JOIN metrics\n    ON (\n        exposures.unit_id = metrics.unit_id\n        AND metrics.timestamp >= \n        \texposures.first_timestamp\n    )\n)\nSELECT \n    group_id,\n    SUM(value) as value\nFROM joined_data\nGROUP BY group_id;\n"})}),"\n",(0,i.jsx)(t.h2,{id:"3-exposure-duplication",children:"3. Exposure duplication"}),"\n",(0,i.jsx)(t.p,{children:"Exposure data must be de-duplicated before joining to ensure a single record per user. Many vendors further manage crossover users (users present in more than one experiment group), removing them from analysis and/or alerting them if this occurs with high frequency."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"SELECT \n    unit_id,\n    experiment_id,\n    MIN(timestamp) as first_timestamp,\n    COUNT(distinct group_id) as groups\nFROM <exposures_table>\nGROUP BY \n    unit_id,\n    experiment_id,\n    group_id\nHAVING COUNT(distinct group_id) = 1;\n"})}),"\n",(0,i.jsx)(t.h2,{id:"4-data-availability",children:"4. Data availability"}),"\n",(0,i.jsxs)(t.p,{children:["When comparing a platform analysis to an ",(0,i.jsx)(t.strong,{children:"existing"})," experiment analysis that may have been run in the past, it's possible that the underlying data has since fallen out of retention or has been otherwise deleted. To check this, you can compare the table's retention policy to the analysis dates used in your original experiment analysis to make sure the data still exists."]}),"\n",(0,i.jsx)(t.h1,{id:"make-sure-results-match",children:"Make sure results match"}),"\n",(0,i.jsxs)(t.p,{children:["After completing the above four steps, your offline analysis should produce results that match those in the Statsig console. Note that the Statsig Console includes several advanced features, such as ",(0,i.jsx)(t.a,{href:"/stats-engine/methodologies/winsorization#winsorization-statsig-whn",children:"winsorization"}),", ",(0,i.jsx)(t.a,{href:"/stats-engine/variance_reduction#cuped",children:"CUPED"}),", and employs ",(0,i.jsx)(t.a,{href:"/stats-engine/variance#ratio-and-mean-metrics",children:"the delta method"})," to address ratio metrics. We recommend disabling these features initially when comparing results."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://www.statsig.com/blog/how-to-analyze-an-experiment-from-databricks-tables",children:"This article"})," provides an example of conducting offline calculations in Databricks."]}),"\n",(0,i.jsx)(t.p,{children:"If you have additional questions, just send us a Slack message. We are always here to help."})]})}function d(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},28453:(e,t,s)=>{s.d(t,{R:()=>r,x:()=>o});var i=s(96540);const n={},a=i.createContext(n);function r(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);