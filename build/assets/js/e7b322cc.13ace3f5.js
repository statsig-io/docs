"use strict";(self.webpackChunkstatsig_docs=self.webpackChunkstatsig_docs||[]).push([[52892],{64106:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var i=n(74848),s=n(28453);const a={title:"Data Best Practices",slug:"/statsig-warehouse-native/guides/best-practices",sidebar_label:"Best Practices",description:"Best Practices for using Statsig in your warehouse",keywords:["owner:vm"],last_update:{date:new Date("2025-09-24T00:00:00.000Z")}},r=void 0,o={id:"statsig-warehouse-native/guides/best-practices",title:"Data Best Practices",description:"Best Practices for using Statsig in your warehouse",source:"@site/docs/statsig-warehouse-native/guides/best-practices.md",sourceDirName:"statsig-warehouse-native/guides",slug:"/statsig-warehouse-native/guides/best-practices",permalink:"/statsig-warehouse-native/guides/best-practices",draft:!1,unlisted:!1,editUrl:"https://github.com/statsig-io/docs/edit/main/docs/statsig-warehouse-native/guides/best-practices.md",tags:[],version:"current",lastUpdatedAt:1758672e6,frontMatter:{title:"Data Best Practices",slug:"/statsig-warehouse-native/guides/best-practices",sidebar_label:"Best Practices",description:"Best Practices for using Statsig in your warehouse",keywords:["owner:vm"],last_update:{date:"2025-09-24T00:00:00.000Z"}},sidebar:"warehouse",previous:{title:"Warehouse Costs",permalink:"/statsig-warehouse-native/guides/costs"},next:{title:"Overview",permalink:"/statsig-warehouse-native/configuration/data-and-semantic-layer"}},l={},d=[{value:"Cost Management",id:"cost-management",level:2},{value:"Follow SQL Best Practices",id:"follow-sql-best-practices",level:3},{value:"Materialize Tables/Views",id:"materialize-tablesviews",level:3},{value:"Use Incremental Reloads",id:"use-incremental-reloads",level:3},{value:"Reload Data Ad-Hoc",id:"reload-data-ad-hoc",level:3},{value:"Leverage Statsig&#39;s Advanced Options",id:"leverage-statsigs-advanced-options",level:3},{value:"Utilize Metric Level Reloads",id:"utilize-metric-level-reloads",level:3},{value:"Use Statsig&#39;s Macros",id:"use-statsigs-macros",level:3},{value:"Avoid Contention",id:"avoid-contention",level:3},{value:"Analytics Optimization",id:"analytics-optimization",level:2},{value:"BigQuery",id:"bigquery",level:3},{value:"Table Layout - Partitioning &amp; Clustering",id:"table-layout---partitioning--clustering",level:4},{value:"Databricks",id:"databricks",level:3},{value:"(Preferred) Use Liquid Clustering",id:"preferred-use-liquid-clustering",level:4},{value:"(Alternative) Partitioning and ZORDER",id:"alternative-partitioning-and-zorder",level:4},{value:"Partition Pruning",id:"partition-pruning",level:4},{value:"Handling Skew and Joins",id:"handling-skew-and-joins",level:4},{value:"Compute Choices",id:"compute-choices",level:4},{value:"Redshift",id:"redshift",level:3},{value:"Table Design - Distribution Style and Sort Key",id:"table-design---distribution-style-and-sort-key",level:4},{value:"Snowflake",id:"snowflake",level:3},{value:"Clustering Keys",id:"clustering-keys",level:4},{value:"Managing High Cardinality Columns",id:"managing-high-cardinality-columns",level:4},{value:"Monitoring Clustering",id:"monitoring-clustering",level:4},{value:"Timestamp Column",id:"timestamp-column",level:4},{value:"Questions?",id:"questions",level:3},{value:"Debugging",id:"debugging",level:2},{value:"Use the Queries from your Statsig console",id:"use-the-queries-from-your-statsig-console",level:3},{value:"Turbo Mode",id:"turbo-mode",level:2},{value:"Ask!",id:"ask",level:3},{value:"Compute Cost Transparency",id:"compute-cost-transparency",level:2}];function c(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"cost-management",children:"Cost Management"}),"\n",(0,i.jsx)(t.p,{children:"Statsig's pipelines leverage many SQL best practices in order to reduce cost, and smaller customers can run month-long analyses for a few pennies."}),"\n",(0,i.jsx)(t.p,{children:"Following these best practices will help keep costs under control and consistent."}),"\n",(0,i.jsxs)(t.p,{children:["Skip ahead to",(0,i.jsx)(t.a,{href:"https://docs.statsig.com/statsig-warehouse-native/guides/best-practices#compute-cost-transparency",children:" Compute Cost Transparency"})," if you want to see how much compute time you experiments use."]}),"\n",(0,i.jsx)(t.h3,{id:"follow-sql-best-practices",children:"Follow SQL Best Practices"}),"\n",(0,i.jsx)(t.p,{children:"Statsig uses your SQL to connect to your source data. Here's some common issues:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsxs)(t.p,{children:["Avoid using ",(0,i.jsx)(t.code,{children:"SELECT *"}),", and only select the columns you'll need"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"SELECT *"})," leads to a lack of clarity in what you are pulling/require for other users"]}),"\n",(0,i.jsx)(t.li,{children:"For warehouse like Bigquery and Snowflake, it can increase the scan/size of materialized assets like CTEs in snowflake"}),"\n",(0,i.jsx)(t.li,{children:"This leads to higher query runtime and higher warehouse bills"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Filter to the data you will need in your base query"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"This reduces the scan and amount of data required in future operations"}),"\n",(0,i.jsx)(t.li,{children:"You can use Statsig Macros (below) to dynamically prune date partitions in subqueries"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Group common metric groups into a single Metric Source"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Statsig will filter your source to the minimal set of data and then create materializations of experiment-tagged data. To maximize the effectiveness of this strategy,\nhave metric sources that cover common suites of metrics that are usually pulled together."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Cluster/Partition tables to reduce query scope"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Most warehouses offer some form of partitioning or indexing along frequently filtered keys. In warehouses with clustering, we recommend the following strategy:","\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Partition/Cluster Assignment Source tables by date, and then your experiment_id column so experiment-level queries can be scoped to just that experiment"}),"\n",(0,i.jsx)(t.li,{children:"Partition/Cluster Metric Source tables based on date, and then the fields you expect to use for filters"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"These best practices are generally true across all warehouses, especially as the datasets you use for experimentation scale."}),"\n",(0,i.jsx)(t.h3,{id:"materialize-tablesviews",children:"Materialize Tables/Views"}),"\n",(0,i.jsx)(t.p,{children:"Since Statsig offers a flexible and robust metric creation flow, it's common for people to write joins or other complex queries in a metric source. As tables and experimental velocity scale, these joins can become expensive since they will be run across every experiment analysis."}),"\n",(0,i.jsx)(t.p,{children:"To reduce the impact of this, best practice is to:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Materialize the results of the join and reference that in Statsig, so you only have to compute the join once"}),"\n",(0,i.jsx)(t.li,{children:"Use Statsig macros to make sure partitions are pruned before the join, and you only join the data you need"}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"use-incremental-reloads",children:"Use Incremental Reloads"}),"\n",(0,i.jsx)(t.p,{children:'Statsig offers both Full and Incremental reloads. Incremental reloads will only process new data, and can be significantly cheaper on long-running experiments. A number of advanced settings in the pulse advanced settings section (and available at an org-level default) will help you make tradeoffs to reduce the total compute cost. For example "Only calculate the latest day of experiment results" skips timeseries, but can run large full reloads (e.g. 1-year on 100M users) in 5 minutes on a Large snowflake cluster.'}),"\n",(0,i.jsx)(t.h3,{id:"reload-data-ad-hoc",children:"Reload Data Ad-Hoc"}),"\n",(0,i.jsx)(t.p,{children:"Depending on your warehouse and data size, Statsig Pulse results can be available in as little as 45 seconds. Since there's flat costs to pipelines, reloading 5 days is not 5 times as expensive as loading one day."}),"\n",(0,i.jsx)(t.p,{children:"If cost is a high concern, being judicious about which results you schedule vs. load on-demand can significantly reduce the amount of processing your warehouse has to do."}),"\n",(0,i.jsx)(t.h3,{id:"leverage-statsigs-advanced-options",children:"Leverage Statsig's Advanced Options"}),"\n",(0,i.jsx)(t.p,{children:"By default, Statsig runs a thorough analysis including historical timeseries and context on your results."}),"\n",(0,i.jsx)(t.h3,{id:"utilize-metric-level-reloads",children:"Utilize Metric Level Reloads"}),"\n",(0,i.jsx)(t.p,{children:"Statsig offers Metric-level reloads; this allows you to add a new metric to an experiment and get its entire history, or restate a single metric after its definition has changed. This is cheaper than a full reload for experiments with many metrics, and is an easy way to check guardrails or analyze follow-up questions post-hoc."}),"\n",(0,i.jsx)(t.h3,{id:"use-statsigs-macros",children:"Use Statsig's Macros"}),"\n",(0,i.jsx)(t.p,{children:"In Metric and Assignment sources, you can use Statsig Macros to directly inject a DATE() type which will be relative to the experiment period being loaded."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.code,{children:"{statsig_start_date}"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.code,{children:"{statsig_end_date}"})}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["For example, in an incremental reload from ",(0,i.jsx)(t.code,{children:"2023-09-01"})," to ",(0,i.jsx)(t.code,{children:"2023-09-03"}),", this query:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"SELECT\n    user_id,\n    event,\n    ts,\n    dt\nFROM log_table\nWHERE dt BETWEEN {statsig_start_date} AND {statsig_end_date}\n"})}),"\n",(0,i.jsx)(t.p,{children:"resolves to"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"SELECT\n    user_id,\n    event,\n    ts,\n    dt\nFROM log_table\nWHERE dt BETWEEN DATE('2023-09-01') AND DATE('2023-09-03')\n"})}),"\n",(0,i.jsx)(t.p,{children:"This is a powerful tool since you can inject filters into queries with joins or CTEs and be confident that the initial scan will be pruned."}),"\n",(0,i.jsx)(t.h3,{id:"avoid-contention",children:"Avoid Contention"}),"\n",(0,i.jsx)(t.p,{children:"Resource contention is a common problem for Data Engineering teams. Usually, there will be large runs in the morning to calculate the previous day's data or reload tables. On warehouses that have flat resources or scaling limits, Pulse queries can be significantly slower during these windows, and additionally will slow down core business logic pipelines."}),"\n",(0,i.jsx)(t.p,{children:"The best practice is to assign a scoped resource to Statsig's service user. This has a few advantages:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Costs are easy to understand, since anything billed to that resource is attributable to Statsig"}),"\n",(0,i.jsx)(t.li,{children:"You can control the max spend by controlling the size of the resource, and independently scale the resource as your experimentation velocity increases"}),"\n",(0,i.jsx)(t.li,{children:"Statsig jobs will not effect your production jobs, and vice versa"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"If this is not possible, it's a good idea to:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Schedule your Statsig runs after your main runs - this also ensures the data in your experiment analysis is fresh"}),"\n",(0,i.jsx)(t.li,{children:"Use API triggers to launch Statsig analyses after the main run is finished"}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"analytics-optimization",children:"Analytics Optimization"}),"\n",(0,i.jsx)(t.p,{children:"When using Statsig\u2019s Metric Explorer to visualize the data within your warehouse, optimizing table layout and clustering configurations can greatly improve latency. This section serves to describe a set of best practices you can employ to improve the performance of analytics queries. Here are recommendations for some of the most commonly used warehouses."}),"\n",(0,i.jsx)(t.h3,{id:"bigquery",children:"BigQuery"}),"\n",(0,i.jsx)(t.h4,{id:"table-layout---partitioning--clustering",children:"Table Layout - Partitioning & Clustering"}),"\n",(0,i.jsx)(t.p,{children:"We advise partitioning on event date and clustering on event when defining your events table. This will improve performance as the majority of queries will filter for the name of the event and the time it was logged. When defining the partition on event date, you should truncate the timestamp to day-level granularity instead of using the raw timestamp (which would otherwise have millisecond precision resulting in very high cardinality)."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Create an events table partitioned by date and clustered by event.\nCREATE TABLE dataset.events (\n  ts TIMESTAMP NOT NULL,\n  event STRING NOT NULL,\n  ...\n)\nPARTITION BY DATE(ts)\nCLUSTER BY event;\n"})}),"\n",(0,i.jsx)(t.p,{children:"BigQuery\u2019s support for applying a cluster to an existing table is limited. Additionally, adding a cluster to an existing table will not automatically recluster the data right away. Given this, if you need to repartition on event date or add a cluster by event, you can create a new table with the correct partitions and clusters using your current table."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Using an existing events table, create a new table that is partitioned by date and clustered by event.\nCREATE OR REPLACE TABLE dataset.events_new\nPARTITION BY DATE(ts)\nCLUSTER BY event\nAS\nSELECT *\nFROM dataset.events;\n\n-- (Optional) Swap the name of your new table with the old one for consistency.\nDROP TABLE dataset.events;\nALTER TABLE dataset.events_new RENAME TO events;\n"})}),"\n",(0,i.jsx)(t.h3,{id:"databricks",children:"Databricks"}),"\n",(0,i.jsx)(t.h4,{id:"preferred-use-liquid-clustering",children:"(Preferred) Use Liquid Clustering"}),"\n",(0,i.jsxs)(t.p,{children:["When making clustering decisions in your events table layout, ",(0,i.jsx)(t.a,{href:"https://docs.databricks.com/aws/en/delta/clustering",children:"liquid clustering"})," provides a flexible approach that allows you to modify your clustering keys without needing to manually rewrite existing data."]}),"\n",(0,i.jsx)(t.p,{children:"We recommend employing liquid clustering for your events table with the following:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Enable liquid clustering for your events table.\nALTER TABLE events SET TBLPROPERTIES ('delta.liquidClustering.enabled' = 'true');\n\n-- Cluster on the event column.\nALTER TABLE events ALTER CLUSTER BY (event);\n\n-- Trigger clustering using the OPTIMIZE command.\nOPTIMIZE events;\n"})}),"\n",(0,i.jsxs)(t.p,{children:["If your events table is frequently being updated, Databricks recommends scheduling ",(0,i.jsx)(t.code,{children:"OPTIMIZE"})," jobs every 1-2 hours. This will incrementally apply liquid clustering to your table."]}),"\n",(0,i.jsx)(t.h4,{id:"alternative-partitioning-and-zorder",children:"(Alternative) Partitioning and ZORDER"}),"\n",(0,i.jsx)(t.p,{children:"If you choose not to use liquid partitioning, our recommendation is to partition on a single low cardinality column such as the date of the event. Try to avoid adding more than one column on the partition unless absolutely necessary and don't partition on a column that has a cardinality that would exceed one thousand. We suggest using a generated column to simplify pruning:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Define a partition on the event date generated column.\nCREATE TABLE events (ts TIMESTAMP, event STRING, event_date DATE GENERATED ALWAYS AS (CAST(ts AS DATE))) USING DELTA PARTITIONED BY (event_date);\n"})}),"\n",(0,i.jsxs)(t.p,{children:["You can use ",(0,i.jsx)(t.code,{children:"ZORDER"})," to colocate similar values within a file for a high cardinality column, which benefits query performance by improving data skipping. We recommend doing this on the event column:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- ZORDER by the event column to improve data skipping.\nOPTIMIZE events ZORDER BY (event);\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Using a scheduled job that runs ",(0,i.jsx)(t.code,{children:"OPTIMIZE"})," on the last week\u2019s event data, you can improve query performance by compacting the number of small data files into fewer, larger files:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Periodically optimize the events table based on the last week of data.\nOPTIMIZE events WHERE event_date >= current_date() - INTERVAL 7 DAYS;\n"})}),"\n",(0,i.jsxs)(t.p,{children:["In general, avoid partitioning on a combination of event date and event. For even small queries, this can create a lot of overhead. Given 500 events, this would cause a 30-day query to hit 15,000 partitions. Instead, partition only by date as described above and ",(0,i.jsx)(t.code,{children:"ZORDER"})," on the event."]}),"\n",(0,i.jsxs)(t.p,{children:["If extra parallelism is desired without introducing too many partitions, you can ",(0,i.jsx)(t.code,{children:"ZORDER"})," by an additional bucket column. This bucket column can be defined as follows:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Add bucket column for extra parallelism.\nALTER TABLE events ADD COLUMN event_bucket INT GENERATED ALWAYS AS (pmod(hash(event), 16));\n\n-- Rewrite data into new files based on this ZORDER key.\nOPTIMIZE events ZORDER BY (event_bucket);\n"})}),"\n",(0,i.jsx)(t.h4,{id:"partition-pruning",children:"Partition Pruning"}),"\n",(0,i.jsx)(t.p,{children:"Dynamic File Pruning will enable Spark to prune partitions based on filter values at runtime. It should be enabled if possible using:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Turn on Dynamic File Pruning.\nSET spark.databricks.optimizer.dynamicFilePruning = true;\n"})}),"\n",(0,i.jsx)(t.h4,{id:"handling-skew-and-joins",children:"Handling Skew and Joins"}),"\n",(0,i.jsx)(t.p,{children:"To rebalance skew and adjust join strategy, we advise turning on Adaptive Query Execution:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Turn on Adapative Query Execution\nSET spark.sql.adaptive.enabled = true;\n"})}),"\n",(0,i.jsx)(t.h4,{id:"compute-choices",children:"Compute Choices"}),"\n",(0,i.jsx)(t.p,{children:"The Photon query engine allows for faster execution of queries with more efficient use of CPU and memory. If possible, it should be enabled for your compute cluster or SQL warehouse."}),"\n",(0,i.jsx)(t.h3,{id:"redshift",children:"Redshift"}),"\n",(0,i.jsx)(t.h4,{id:"table-design---distribution-style-and-sort-key",children:"Table Design - Distribution Style and Sort Key"}),"\n",(0,i.jsx)(t.p,{children:"As Redshift does not support partitioning by column, we can instead employ use of a sort key. Given most event tables are generally append-only and time-based, we advise use of a compound sort key on (timestamp, event) so that the data is ordered by time. Given all analytics queries will filter on timestamp, this should allow for queries to read only the relevant blocks."}),"\n",(0,i.jsx)(t.p,{children:"Ordering secondarily by event will mean rows are grouped by event within each time block, which can reduce scan size when filtering by event. We want pruning to be primarily upon timestamp, which is why it is kept as the first key."}),"\n",(0,i.jsx)(t.p,{children:"Given there can be significant skew among event types, we generally advise using an automatic distribution style rather than distributing upon the event column. This will allow Redshift to make distribution decisions based on the size of the table and query patterns. You can create an events table with the above recommendations in mind as follows:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Create an events table with an automatic distrbution style and sort key on timestamp, event.\nCREATE TABLE events (\n  event       VARCHAR NOT NULL,\n  ts          TIMESTAMP     NOT NULL,\n\t...\n)\nDISTSTYLE AUTO\nCOMPOUND SORTKEY (ts, event);\n"})}),"\n",(0,i.jsx)(t.p,{children:"If you already have an events table and want to leverage the recommended distribution style and sort key, you won\u2019t be able to apply those changes by modifying the existing table. Instead, you can create a new table and copy over data:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Create a new events table with the preferred distribution style and sort key.\nCREATE TABLE events_new (\n  event       VARCHAR NOT NULL,\n  ts          TIMESTAMP     NOT NULL,\n\t...\n)\nDISTSTYLE AUTO\nCOMPOUND SORTKEY (ts, event);\n\n-- Copy over data from the old events table.\nINSERT INTO events_new (event, ts, ...)\nSELECT event_id, ts, ...\nFROM events;\n\n-- Rename the two tables in a single transaction.\nBEGIN;\nALTER TABLE events RENAME TO events_old;\nALTER TABLE events_new RENAME TO events;\nCOMMIT;\n\n-- Drop the old table.\nDROP TABLE events_old;\n"})}),"\n",(0,i.jsx)(t.h3,{id:"snowflake",children:"Snowflake"}),"\n",(0,i.jsx)(t.h4,{id:"clustering-keys",children:"Clustering Keys"}),"\n",(0,i.jsx)(t.p,{children:"Given Snowflake does not allow for explicit partitioning, we recommend using clustering keys on the event date and event to improve performance as the majority of queries will filter for the name of the event and the time it was logged. Note that we advise clustering on the timestamp truncated to day-level granularity instead of the raw timestamp (which would otherwise have millisecond precision causing very high cardinality)."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Cluster events on the timestamp truncated to day and event.\nALTER TABLE events CLUSTER BY (DATE_TRUNC('day', ts), event);\n"})}),"\n",(0,i.jsxs)(t.p,{children:["If you have a high cardinality of unique event types, it might be advisable to add a search optimization on event instead of clustering by that column. See the ",(0,i.jsx)(t.a,{href:"#managing-high-cardinality-columns",children:"next section"})," for details."]}),"\n",(0,i.jsx)(t.h4,{id:"managing-high-cardinality-columns",children:"Managing High Cardinality Columns"}),"\n",(0,i.jsx)(t.p,{children:"If there are high cardinality columns that you frequently reference in Metrics Explorer filters, consider using search optimization rather than clustering by it. This avoids an overly large range of values for each micro-partition generated by Snowflake, which would make pruning inefficient. For high cardinality columns, we can instead rely on the auxiliary search index generated through search optimization."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Add search optimization for fast lookups.\nALTER TABLE events ADD SEARCH OPTIMIZATION ON EQUALITY(some_column);\n"})}),"\n",(0,i.jsx)(t.h4,{id:"monitoring-clustering",children:"Monitoring Clustering"}),"\n",(0,i.jsxs)(t.p,{children:["You can use ",(0,i.jsx)(t.code,{children:"SYSTEM$CLUSTERING_INFORMATION"})," to check if your current clustering scheme is effective. Large values of average_depth and average_overlaps may indicate the existing table should be reclustered on different keys with lower cardinality."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"-- Given an events table clustered on event date and event, check the clustering information.\nSELECT SYSTEM$CLUSTERING_INFORMATION('YOUR_DATABASE.YOUR_SCHEMA.EVENTS', '(DATE_TRUNC(''day'', ts), event)');\n"})}),"\n",(0,i.jsx)(t.h4,{id:"timestamp-column",children:"Timestamp Column"}),"\n",(0,i.jsxs)(t.p,{children:["If possible, use the ",(0,i.jsx)(t.code,{children:"TIMESTAMP_NTZ"})," (no timezone) type for your timestamp column. This saves on space as the offset metadata will not need to be stored. It additionally can help speed up queries as it allows Snowflake to avoid the extra work for timezone normalization internally, which would otherwise take place during filtering and pruning of micro-partitions."]}),"\n",(0,i.jsx)(t.h3,{id:"questions",children:"Questions?"}),"\n",(0,i.jsx)(t.p,{children:"If you need additional support in optimizing your warehouse configuration for analytics, please reach out in the Slack support channel for your organization within Statsig Connect."}),"\n",(0,i.jsx)(t.h2,{id:"debugging",children:"Debugging"}),"\n",(0,i.jsx)(t.p,{children:"Statsig shows you all the SQL being run, and any errors that occur. Generally these are caused by changing underlying tables or Metric Sources, causing a metric query to fail. Here's some best practices for debugging Statsig Queries."}),"\n",(0,i.jsx)(t.h3,{id:"use-the-queries-from-your-statsig-console",children:"Use the Queries from your Statsig console"}),"\n",(0,i.jsx)(t.p,{children:"If a pulse load fails, you can find all of the SQL queries and associated error messages in the Diagnostics tab. You can easily click the copy button to run/debug the query within your warehouse. Look out for common errors:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"A query attempting to access a field that no longer exists on a table"}),"\n",(0,i.jsx)(t.li,{children:"A table not existing - usually due to the Statsig user not having permission on a new table"}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"turbo-mode",children:"Turbo Mode"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"../features/turbo",children:"Turbo Mode"})," skips some enrichment calculations (in particular some time series rollups) in order to very cheaply compute the latest snapshot of your data. With this, customers have run experiments on 150+ million users in less than 5 minutes on a snowflake S cluster."]}),"\n",(0,i.jsx)(t.h3,{id:"ask",children:"Ask!"}),"\n",(0,i.jsx)(t.p,{children:"Statsig's support is very responsive, and will be happy to help you fix your issue and build tools to prevent it in the future - whether it's due to system or user error."}),"\n",(0,i.jsx)(t.h2,{id:"compute-cost-transparency",children:"Compute Cost Transparency"}),"\n",(0,i.jsx)(t.p,{children:"Statsig Warehouse Native now lets you get a birds eye view across the compute time experiment analysis incurs in your warehouse. Break this down by experiment, metric source or type of query to find what to optimize.\nCommon customers we've designed the dashboard to be able to address include\nWhat Metric Sources take the most compute time (useful to focus optimization effort here; see tips here)\nWhat is the split of compute time between full loads vs incremental loads vs custom queries?\nHow is compute time distributed across experiments? (useful to make sure value realized and compute costs incurred are roughly aligned)"}),"\n",(0,i.jsxs)(t.p,{children:["You can find this dashboard in the Left Nav under Analytics -> Dashboards -> Pipeline Overview\n",(0,i.jsx)(t.img,{src:"https://github.com/user-attachments/assets/684ae633-8054-4f41-8443-7df63fe81253",alt:"Pipeline Overview dashboard interface"})]}),"\n",(0,i.jsx)(t.p,{children:"This is built using Statsig Product Analytics - you can customize any of these charts, or build new ones yourself. A favorite is to add in your average compute cost, so you can turn slot time per experiment into $ cost per experiment."}),"\n",(0,i.jsxs)(t.p,{children:["At the end of every Pulse load / DAG, we'll upload a single row to the ",(0,i.jsx)(t.code,{children:"pipeline_overview"})," table for each job executed as part of that run. This table has the following schema:"]}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Column"}),(0,i.jsx)(t.th,{children:"Type"}),(0,i.jsx)(t.th,{children:"Description"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"ts"}),(0,i.jsx)(t.td,{children:"timestamp"}),(0,i.jsx)(t.td,{children:"Timestamp at which the DAG was created."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"job_type"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsxs)(t.td,{children:["Job type (see ",(0,i.jsx)(t.a,{href:"https://docs.statsig.com/statsig-warehouse-native/pipeline-overview/",children:"Pipeline Overview"}),")"]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"metric_source_id"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"Only applicable for 'Unit-Day Calculations' jobs - the ID of the metric source"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"assignment_source_id"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"The Assignment Source ID of the experiment for which Pulse was loaded."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"job_status"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsxs)(t.td,{children:["The final state of the job (",(0,i.jsx)(t.code,{children:"fail"})," or ",(0,i.jsx)(t.code,{children:"success"}),")"]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"metrics"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"Metrics processed by the job"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"dag_state"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsxs)(t.td,{children:["Final state of the DAG (",(0,i.jsx)(t.code,{children:"success"}),", ",(0,i.jsx)(t.code,{children:"partial_failure"}),", or ",(0,i.jsx)(t.code,{children:"failure"}),")"]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"dag_type"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsxs)(t.td,{children:["Type of DAG (",(0,i.jsx)(t.code,{children:"full"}),", ",(0,i.jsx)(t.code,{children:"incremental"}),", ",(0,i.jsx)(t.code,{children:"metric"}),", ",(0,i.jsx)(t.code,{children:"power"}),", ",(0,i.jsx)(t.code,{children:"custom_query"}),", ",(0,i.jsx)(t.code,{children:"autotune"}),", ",(0,i.jsx)(t.code,{children:"assignment_source"}),", ",(0,i.jsx)(t.code,{children:"stratified_sampling"}),")"]})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"experiment_id"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"ID of the experiment for which Pulse was loaded, if applicable"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"dag_start_ds"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"Start of the date range being loaded"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"dag_end_ds"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"End of the date range being loaded"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"wall_time"}),(0,i.jsx)(t.td,{children:"number"}),(0,i.jsx)(t.td,{children:"Total time elapsed between DAG start and finish, in milliseconds"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"turbo_mode"}),(0,i.jsx)(t.td,{children:"boolean"}),(0,i.jsx)(t.td,{children:"Whether the DAG was run in Turbo Mode"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"dag_id"}),(0,i.jsx)(t.td,{children:"string"}),(0,i.jsx)(t.td,{children:"Internal identifier for the DAG"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"dag_duration"}),(0,i.jsx)(t.td,{children:"number"}),(0,i.jsx)(t.td,{children:"Number of days in the date range being loaded"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"is_scheduled"}),(0,i.jsx)(t.td,{children:"boolean"}),(0,i.jsx)(t.td,{children:"Whether the DAG was triggered by a scheduled run"})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>o});var i=n(96540);const s={},a=i.createContext(s);function r(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);