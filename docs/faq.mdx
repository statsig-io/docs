---
title: FAQ
slug: /faq
sidebar_label: FAQ
---

For additional guidance, explore this page, where we've compiled answers to common questions. We'll continue to update this list and integrate relevant answers into the documentation as needed.

## SDKs and APIs

## How does bucketing within the Statsig SDKs work?

Bucketing on Statsig is **deterministic**. Given the same user object and the same state of the experiment or feature gate, Statsig will always evaluate to the same result. This holds true even if the evaluation happens in different places (e.g., client or server).

Here’s a high-level breakdown of how bucketing works:
1. **Salt Creation**: A unique salt is generated for each experiment or feature gate.
2. **Hashing**: The unit (e.g., `userId`, `organizationId`) is passed through a hashing function (SHA256) along with the unique salt, which produces a big integer.
3. **Bucket Assignment**: The big integer is subjected to a modulus operation with 10000, resulting in a value between 0 and 9999. (For layers, it uses 1000.)
4. **Determination**: The result represents the specific bucket assigned to the unit, ensuring deterministic bucketing across experiments or feature gates.

This process ensures randomized yet deterministic bucketing, and the unique salt ensures that the same unit can be assigned to different buckets across different experiments.

For more details, check out our open-source SDKs [here](https://github.com/statsig-io/node-js-server-sdk/blob/main/src/Evaluator.ts).

> **Note**: We don't store or track which IDs passed a feature gate or were assigned to an experiment group. Keeping distributed evaluation state across clients and servers doesn’t scale. In fact, some customers previously paid more to maintain Redis instances for tracking states than they did for Statsig!

---

## Is it possible to add a layer to a running experiment?

Once an experiment has started, you cannot change its layer. Doing so would compromise the integrity of the experiment by reshuffling existing assignments. While you can't modify layers on running experiments, this feature may be supported in the future.

---

## Why should I define parameters for my experiments and get them in code, instead of just getting the group?

Defining **parameters** for experiments allows for faster iteration and greater flexibility in experiment design—without requiring code changes for every new experiment.

For example, if you want to test button colors:

**Traditional Group-based Approach**:
```js
if (otherExpEngine.getExperiment('button_color_test').getGroup() === 'Control') {
    color = 'BLACK';
} else if (otherExpEngine.getExperiment('button_color_test').getGroup() === 'Blue') {
    color = 'BLUE';
}
```

**Parameter-based Approach with Statsig**:
```js
color = statsig.getExperiment('button_color_test').getString('button_color', 'BLACK');
```

In the first case, adding a new color (e.g., "GREEN") requires a code change and a new release. With Statsig's parameterized approach, you can add a new group that returns "GREEN" in your experiment settings—**no code change or release cycle needed**.

---

## Why am I not seeing my exposures and custom events logged in Statsig?

If Statsig is executing in a **short-lived process** (e.g., a script or edge worker environment), it’s likely that the process exits before the event queue is flushed. To ensure all events are sent to Statsig, call `statsig.flush()` before the process exits.

Some edge providers offer utilities to ensure events are flushed without blocking the response to the client. [Learn more](/server/nodejsServerSDK#flushing-events).

Additionally, if a specific custom event is missing, check whether its name is valid. Statsig drops events matching this regex or containing invalid characters: `"\\[\]{}<>#=;&$%|\u0000\n\r"`.

---

## I don't see my client or server language listed, can I still use Statsig?

If none of our current SDKs fit your needs, let us know in [Slack](https://statsig.com/slack)!

---

# Feature Flags

## When I increase/decrease the rollout percentage of a rule on a feature gate, will users who were passing the rule continue to pass?

Yes. For example:
- If the rule was passing 10% of users and you increase it to 20%, the original 10% will continue to pass, and an additional 10% will now pass.
- If you reduce the percentage from 20% back to 10%, the original 10% of passing users will still pass. To reshuffle users, you'll need to "resalt" the gate.

---

# Statistics

## What statistical tests does Statsig use?

Statsig primarily uses a **two-sample Z-test** for most experiments and **Welch’s t-test** for smaller samples. This combination is based on extensive literature research and simulations, ensuring trustworthy and reliable results.

For more details, see how we [calculate p-values](/stats-engine/p-value).

---

## How does Statsig handle low sample size?

For small sample sizes, Statsig uses [Welch's t-test](/stats-engine/p-value#welchs-t-test) to handle unequal sample sizes or variances while maintaining low false positive rates.

We also offer:
- **CUPED**: A powerful tool that leverages pre-experiment data to reduce variance and increase experiment velocity.
- **Winsorization**: A method to limit the impact of outliers and stabilize results in small sample experiments.

---

## How does Statsig deal with ‘the peeking problem’?

Statsig uses **mSPRT** for [sequential testing](/experiments-plus/sequential-testing). This method dynamically adjusts the confidence interval based on the variance and sample size, ensuring a controlled false positive rate while detecting significant effects early.

---

## What is CUPED?

[CUPED](/stats-engine/methodologies/cuped) stands for **Controlled-experiment Using Pre-Experiment Data**. It’s a technique that reduces the variance of estimates by using pre-experiment data as a covariate in the analysis. This boosts the speed and accuracy of experiments.

---

## How can I reduce variance / increase power / accelerate decision making?

You can reduce variance and improve decision-making by using:
- **CUPED**: Leverage pre-experiment data.
- **Winsorization**: Limit the influence of outliers on results.
- **MAB (Autotune)**: Dynamically allocate traffic to optimize your chosen metric, accelerating decision-making.

---

# Experimentation

## How can I get started with running an A/B Test?

To run a simple A/B test:
- If your feature isn't live yet, create a **partial rollout** using a [feature flag](/guides/first-feature). This will create test and control groups.
- If your feature is already in production, create an [experiment](/guides/abn-tests) to test different variants.

You can analyze results using the **Pulse Results** tab in your feature gate or experiment.

---

## Can I target my experiment at a subset of users (e.g., iOS users)?

Yes! When setting up an experiment, you can use a **Feature Flag** to target users who meet specific criteria, such as iOS users.

![iOS Targeting Example](https://user-images.githubusercontent.com/31516123/229173350-a1795b5a-bdef-4c58-afb1-36806dc38f1a.png)

Learn more about working with feature flags [here](/feature-flags/working-with).

---

# Billing

## What counts as a [billable event](https://statsig.com/pricing#faq)?

A **billable event** occurs when:
- Your application calls the Statsig SDK to evaluate a feature flag or experiment.
- Events are logged (e.g., exposures or custom metrics).

Statsig deduplicates exposure events within 60 minutes on the client and within 1 minute on the server to avoid over-counting.

---

## How do I manage my billable event volume?

To manage billable event volume:
1. **Download CSVs**: Go to the Usage and Billing tab in Project Settings to download a CSV of event usage.
2. **Filter in Console**: Explore which gates/experiments consume the most events.
3. **Set Alerts**: Admins are alerted at 25%, 50%, 75%, and 100% of event usage.

![Event Usage](https://user-images.githubusercontent.com/31516123/218547079-ba33751c-43f0-4ff7-8733-501711596333.png)

For more details, check the [Console API](/console-api/usage-billing) for downloading usage data.

---

# Platform Usability

## When should I create a new project?

Create a new project when:
- You're managing a distinct product with its own user IDs and metrics.
- If you need to track success across platforms (e.g., web, iOS, Android), use a single project to consolidate user metrics like DAU.

If you have distinct use cases (e.g., a marketing site for anonymous users vs. a product for signed-in users), create separate projects to avoid overlap.

---

## What domains do I need to whitelist for Content Security Policy (CSP)?

Whitelist the following domains in your CSP configuration:
- `https://featuregates.org`: Used by SDKs to initialize and download

 configuration.
- `https://events.statsigapi.net`: Used by SDKs to log events.

---
