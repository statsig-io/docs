---
title: Data Modelling
slug: /statsig-warehouse-native/building-blocks/data-modelling
sidebar_label: Data Modelling
---

For most customers, Pulse results can be calculated for pennies and in a few minutes. However, as your data scales and for longer experiments such as holdouts, it's worth making sure your data is structured well for analysis with Statsig.

This page has some best practices for working with Statsig Warehouse Native:

### Use materialized tables for complex results

If your analysis relies on complex joins that you'll be usign across multiple experiments, the best practice is to create a table or artifact to store the result of that operation. In this way, the expensive operation only needs to be done once, instead of once per Experiment.

This is also a good practice if you have metrics like server-side logging that have lots of rows. Grouping at the user-day level can decrease the data load per experiment analysis an order of magnitude.

### Create a single source for multiple metrics if those will often be used together

If you have a set of metrics from the same table or dataset that are frequently used in the same experiment, it makes sense to make one metric source that they all pull from -- you can add additional filters in the Metric itself. For example, if you have these two metrics:

- Revenue
- Non-refunded revenue

And they are derived from two columns in the same table, Statsig will optimize its join Strategy to allow you to only run one join for both metrics.

### Cluster/Partition tables to reduce query scope.

We recommend partitioning all of your source tables by date. In warehouses with clustering, we recommend the following strategy:

- Cluster Assignment Source tables by your `experiment_id` column so experiment-level queries can be scoped to just that experiment
- Cluster Metric Source tables based on the fields you expect to use for filters

### Size Compute Resources Appropriately

Experiment analysis does require large joins and aggregations in order to calculate results. If a resource node is too small, this leads to a large amount of Spill as the cluster runs out of memory and has to write to local disk or across network. It will be faster - and often cheaper - to use a larger compute cluster to avoid spilling data.
