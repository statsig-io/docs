---
title: Prompt Management

sidebar_label: Prompts
slug: /ai-evals/prompts

keywords:
  - owner:vm
last_update:
  date: 2025-08-09
---


## What is a Prompt in Statsig?
A Prompt is a way to represent an LLM prompt or a task in Statsig, with it's config. Prompts are similar to Dynamic Configs, and allow you to evaluate and roll out prompts in production without deploying code. You can use the Statsig [Node](/server-core/node-core#getting-a-prompt) or [Python](/server-core/python-core/#getting-a-prompt) Server Core SDKs to retrieve this prompt within your app at runtime and use it. 

With Prompts, you can 
- Manage your prompt configuration outside of your application code. You can update model, configuration or prompt at runtime. 
- Team mates who have access to Statsig can collaborate and iterate on prompts, while benefitting from Statsig's production change control processes and versioning. 
- Add configuration for a new model, model provider and progressively shift production traffic to this while comparing costs, user satisfaction or any metric of interest. 
- Support advanced use cases such as
   - retrieval-augmented generation (RAG) and
   - evaluation in production.
<figure>
<img alt="image" src="https://github.com/user-attachments/assets/77256d12-0d3f-4caa-8fa7-3719517af4eb" />
<figcaption>Creating a prompt</figcaption>
</figure>

<figure>
<img alt="image" src="https://github.com/user-attachments/assets/66e3b7f6-4613-4d7c-84c7-0fd84f890a29" />
<figcaption>Code snippet to retrieve the Live version of the prompt</figcaption>
</figure>

<figure>
<img alt="image" src="https://github.com/user-attachments/assets/546c4974-b01d-4d13-85a1-4f2a6bfbb6c1" />
<figcaption>Looking at the scores for a prompt version</figcaption>
</figure>
