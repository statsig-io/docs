{
  "id": "autotune/contextual/introduction",
  "title": "Contextual Bandit (Autotune AI)",
  "description": "Contextual Multi-Armed Bandits are a subset of Multi-Armed-Bandits which use context about a user to personalize their experience. This is generally achieved by predicting outcomes from among the variants, and picking the best outcome while factoring for uncertainty. Specifically, they will tend to prefer a slightly worse prediction that has a lot of uncertainty, thereby exploring that variant.",
  "source": "@site/docs/autotune/contextual/introduction.md",
  "sourceDirName": "autotune/contextual",
  "slug": "/autotune/contextual/introduction",
  "permalink": "/autotune/contextual/introduction",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/statsig-io/docs/edit/main/docs/autotune/contextual/introduction.md",
  "tags": [],
  "version": "current",
  "lastUpdatedAt": 1758153600000,
  "frontMatter": {
    "title": "Contextual Bandit (Autotune AI)",
    "sidebar_label": "Contextual Bandit",
    "slug": "/autotune/contextual/introduction",
    "keywords": [
      "owner:vm"
    ],
    "last_update": {
      "date": "2025-09-18T00:00:00.000Z"
    }
  },
  "sidebar": "cloud",
  "previous": {
    "title": "Methodology",
    "permalink": "/multi-armed-bandit"
  },
  "next": {
    "title": "Getting Started",
    "permalink": "/autotune/contextual/getting-started"
  }
}